<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/blog/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/blog/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/blog/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="spark,大数据," />








  <link rel="shortcut icon" type="image/x-icon" href="/blog/favicon.ico?v=5.1.0" />






<meta name="description" content="记录一下sparksql的dataframe 中常用的操作，spark在大数据处理方面有很广泛的应供，每天都在研究spark的源码，简单记录一下以便后续查阅,今天先简单整理一下，后续逐步完善.版本:spark 2.0.1">
<meta name="keywords" content="spark,大数据">
<meta property="og:type" content="article">
<meta property="og:title" content="spark 源码分析之 sparksql DataSet">
<meta property="og:url" content="https://hxfeng.github.io/2017/03/11/spark1/index.html">
<meta property="og:site_name" content="在路上">
<meta property="og:description" content="记录一下sparksql的dataframe 中常用的操作，spark在大数据处理方面有很广泛的应供，每天都在研究spark的源码，简单记录一下以便后续查阅,今天先简单整理一下，后续逐步完善.版本:spark 2.0.1">
<meta property="og:locale" content="zh">
<meta property="og:updated_time" content="2018-05-29T10:13:34.626Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="spark 源码分析之 sparksql DataSet">
<meta name="twitter:description" content="记录一下sparksql的dataframe 中常用的操作，spark在大数据处理方面有很广泛的应供，每天都在研究spark的源码，简单记录一下以便后续查阅,今天先简单整理一下，后续逐步完善.版本:spark 2.0.1">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/blog/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://hxfeng.github.io/2017/03/11/spark1/"/>





  <title> spark 源码分析之 sparksql DataSet | 在路上 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh">

  














  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/blog/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">在路上</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">人不能太舒服，太舒服就会有问题，so解决问题ing</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/blog/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/blog/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/blog/About" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/blog/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/blog/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://hxfeng.github.io/blog/2017/03/11/spark1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="hxfeng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/blog/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="在路上">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                spark 源码分析之 sparksql DataSet
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-03-11T00:00:00+08:00">
                2017-03-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/blog/2017/03/11/spark1/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/03/11/spark1/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> 浏览
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>次
            </span>
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>记录一下sparksql的dataframe 中常用的操作，spark在大数据处理方面有很广泛的应供，每天都在研究spark的源码，简单记录一下以便后续查阅,今天先简单整理一下，后续逐步完善.<br>版本:spark 2.0.1<br><a id="more"></a></p>
<h2 id="数据显示"><a href="#数据显示" class="headerlink" title="数据显示"></a>数据显示</h2><p>这个showString 是spark内部的方法，我们实际是调用不到的，但是我们调用的show方法最终都是调用了这个showString<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Compose the string representing rows for output</span><br><span class="line"> *</span><br><span class="line"> * @param _numRows Number of rows to show</span><br><span class="line"> * @param truncate If set to more than 0, truncates strings to `truncate` characters and</span><br><span class="line"> *                   all cells will be aligned right.</span><br><span class="line"> */</span><br><span class="line">private[sql] def showString(_numRows: Int, truncate: Int = 20): String</span><br></pre></td></tr></table></figure></p>
<h2 id="将dataSet转换成dataFrame"><a href="#将dataSet转换成dataFrame" class="headerlink" title="将dataSet转换成dataFrame"></a>将dataSet转换成dataFrame</h2><p>   datafram其实是按列来存储的dataset<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed.</span><br><span class="line"> * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with</span><br><span class="line"> * meaningful names. For example:</span><br><span class="line"> * &#123;&#123;&#123;</span><br><span class="line"> *   val rdd: RDD[(Int, String)] = ...</span><br><span class="line"> *   rdd.toDF()  // this implicit conversion creates a DataFrame with column name `_1` and `_2`</span><br><span class="line"> *   rdd.toDF(&quot;id&quot;, &quot;name&quot;)  // this creates a DataFrame with column name &quot;id&quot; and &quot;name&quot;</span><br><span class="line"> * &#125;&#125;&#125;</span><br><span class="line"> *</span><br><span class="line"> * @group basic</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">@scala.annotation.varargs</span><br><span class="line">def toDF(colNames: String*): DataFrame = &#123;</span><br><span class="line">  require(schema.size == colNames.size,</span><br><span class="line">    &quot;The number of columns doesn&apos;t match.\n&quot; +</span><br><span class="line">      s&quot;Old column names ($&#123;schema.size&#125;): &quot; + schema.fields.map(_.name).mkString(&quot;, &quot;) + &quot;\n&quot; +</span><br><span class="line">      s&quot;New column names ($&#123;colNames.size&#125;): &quot; + colNames.mkString(&quot;, &quot;))</span><br><span class="line"></span><br><span class="line">  val newCols = logicalPlan.output.zip(colNames).map &#123; case (oldAttribute, newName) =&gt;</span><br><span class="line">    Column(oldAttribute).as(newName)</span><br><span class="line">  &#125;</span><br><span class="line">  select(newCols : _*)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>输出当前dataset的结构信息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Returns the schema of this Dataset.</span><br><span class="line"> *</span><br><span class="line"> * @group basic</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">def schema: StructType = queryExecution.analyzed.schema</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Prints the schema to the console in a nice tree format.</span><br><span class="line"> *</span><br><span class="line"> * @group basic</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">// scalastyle:off println</span><br><span class="line">def printSchema(): Unit = println(schema.treeString)</span><br><span class="line">// scalastyle:on println</span><br></pre></td></tr></table></figure></p>
<p>输出一些调试信息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Prints the plans (logical and physical) to the console for debugging purposes.</span><br><span class="line"> *</span><br><span class="line"> * @group basic</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">def explain(extended: Boolean): Unit = &#123;</span><br><span class="line">  val explain = ExplainCommand(queryExecution.logical, extended = extended)</span><br><span class="line">  sparkSession.sessionState.executePlan(explain).executedPlan.executeCollect().foreach &#123;</span><br><span class="line">    // scalastyle:off println</span><br><span class="line">    r =&gt; println(r.getString(0))</span><br><span class="line">    // scalastyle:on println</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Prints the physical plan to the console for debugging purposes.</span><br><span class="line"> *</span><br><span class="line"> * @group basic</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">def explain(): Unit = explain(extended = false)</span><br></pre></td></tr></table></figure></p>
<p>输出列名以及每个列的类型<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Returns all column names and their data types as an array.</span><br><span class="line"> *</span><br><span class="line"> * @group basic</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">def dtypes: Array[(String, String)] = schema.fields.map &#123; field =&gt;</span><br><span class="line">  (field.name, field.dataType.toString)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Returns all column names as an array.</span><br><span class="line"> *</span><br><span class="line"> * @group basic</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">def columns: Array[String] = schema.fields.map(_.name)</span><br></pre></td></tr></table></figure></p>
<p>是否能够获取数据<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Returns true if the `collect` and `take` methods can be run locally</span><br><span class="line"> * (without any Spark executors).</span><br><span class="line"> *</span><br><span class="line"> * @group basic</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation]</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Returns true if this Dataset contains one or more sources that continuously</span><br><span class="line"> * return data as it arrives. A Dataset that reads data from a streaming source</span><br><span class="line"> * must be executed as a `StreamingQuery` using the `start()` method in</span><br><span class="line"> * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or</span><br><span class="line"> * `collect()`, will throw an [[AnalysisException]] when there is a streaming</span><br><span class="line"> * source present.</span><br><span class="line"> *</span><br><span class="line"> * @group streaming</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">@Experimental</span><br><span class="line">@InterfaceStability.Evolving</span><br><span class="line">def isStreaming: Boolean = logicalPlan.isStreaming</span><br></pre></td></tr></table></figure></p>
<p><strong>检查点，以前没有用过，需要在研究一下</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate</span><br><span class="line"> * the logical plan of this Dataset, which is especially useful in iterative algorithms where the</span><br><span class="line"> * plan may grow exponentially. It will be saved to files inside the checkpoint</span><br><span class="line"> * directory set with `SparkContext#setCheckpointDir`.</span><br><span class="line"> *</span><br><span class="line"> * @group basic</span><br><span class="line"> * @since 2.1.0</span><br><span class="line"> */</span><br><span class="line">@Experimental</span><br><span class="line">@InterfaceStability.Evolving</span><br><span class="line">def checkpoint(): Dataset[T] = checkpoint(eager = true)</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the</span><br><span class="line"> * logical plan of this Dataset, which is especially useful in iterative algorithms where the</span><br><span class="line"> * plan may grow exponentially. It will be saved to files inside the checkpoint</span><br><span class="line"> * directory set with `SparkContext#setCheckpointDir`.</span><br><span class="line"> *</span><br><span class="line"> * @group basic</span><br><span class="line"> * @since 2.1.0</span><br><span class="line"> */</span><br><span class="line">@Experimental</span><br><span class="line">@InterfaceStability.Evolving</span><br><span class="line">def checkpoint(eager: Boolean): Dataset[T] = &#123;</span><br><span class="line">  val internalRdd = queryExecution.toRdd.map(_.copy())</span><br><span class="line">  internalRdd.checkpoint()</span><br><span class="line"></span><br><span class="line">  if (eager) &#123;</span><br><span class="line">    internalRdd.count()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  val physicalPlan = queryExecution.executedPlan</span><br></pre></td></tr></table></figure></p>
<p>这个什么鬼需要在分析一下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">  // Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the</span><br><span class="line">  // size of `PartitioningCollection` may grow exponentially for queries involving deep inner</span><br><span class="line">  // joins.</span><br><span class="line">  def firstLeafPartitioning(partitioning: Partitioning): Partitioning = &#123;</span><br><span class="line">    partitioning match &#123;</span><br><span class="line">      case p: PartitioningCollection =&gt; firstLeafPartitioning(p.partitionings.head)</span><br><span class="line">      case p =&gt; p</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  val outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning)</span><br><span class="line"></span><br><span class="line">  Dataset.ofRows(</span><br><span class="line">    sparkSession,</span><br><span class="line">    LogicalRDD(</span><br><span class="line">      logicalPlan.output,</span><br><span class="line">      internalRdd,</span><br><span class="line">      outputPartitioning,</span><br><span class="line">      physicalPlan.outputOrdering</span><br><span class="line">    )(sparkSession)).as[T]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>水印？？？<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * :: Experimental ::</span><br><span class="line"> * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time</span><br><span class="line"> * before which we assume no more late data is going to arrive.</span><br><span class="line"> *</span><br><span class="line"> * Spark will use this watermark for several purposes:</span><br><span class="line"> *  - To know when a given time window aggregation can be finalized and thus can be emitted when</span><br><span class="line"> *    using output modes that do not allow updates.</span><br><span class="line"> *  - To minimize the amount of state that we need to keep for on-going aggregations,</span><br><span class="line"> *    `mapGroupsWithState` and `dropDuplicates` operators.</span><br><span class="line"> *</span><br><span class="line"> *  The current watermark is computed by looking at the `MAX(eventTime)` seen across</span><br><span class="line"> *  all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost</span><br><span class="line"> *  of coordinating this value across partitions, the actual watermark used is only guaranteed</span><br><span class="line"> *  to be at least `delayThreshold` behind the actual event time.  In some cases we may still</span><br><span class="line"> *  process records that arrive more than `delayThreshold` late.</span><br><span class="line"> *</span><br><span class="line"> * @param eventTime the name of the column that contains the event time of the row.</span><br><span class="line"> * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest</span><br><span class="line"> *                       record that has been processed in the form of an interval</span><br><span class="line"> *                       (e.g. &quot;1 minute&quot; or &quot;5 hours&quot;).</span><br><span class="line"> *</span><br><span class="line"> * @group streaming</span><br><span class="line"> * @since 2.1.0</span><br><span class="line"> */</span><br><span class="line">@Experimental</span><br><span class="line">@InterfaceStability.Evolving</span><br><span class="line">// We only accept an existing column name, not a derived column here as a watermark that is</span><br><span class="line">// defined on a derived column cannot referenced elsewhere in the plan.</span><br><span class="line">def withWatermark(eventTime: String, delayThreshold: String): Dataset[T] = withTypedPlan &#123;</span><br><span class="line">  val parsedDelay =</span><br><span class="line">    Option(CalendarInterval.fromString(&quot;interval &quot; + delayThreshold))</span><br><span class="line">      .getOrElse(throw new AnalysisException(s&quot;Unable to parse time delay &apos;$delayThreshold&apos;&quot;))</span><br><span class="line">  EventTimeWatermark(UnresolvedAttribute(eventTime), parsedDelay, logicalPlan)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="数据展示"><a href="#数据展示" class="headerlink" title="数据展示"></a>数据展示</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">  /**</span><br><span class="line">   * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated,</span><br><span class="line">   * and all cells will be aligned right. For example:</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   year  month AVG(&apos;Adj Close) MAX(&apos;Adj Close)</span><br><span class="line">   *   1980  12    0.503218        0.595103</span><br><span class="line">   *   1981  01    0.523289        0.570307</span><br><span class="line">   *   1982  02    0.436504        0.475256</span><br><span class="line">   *   1983  03    0.410516        0.442194</span><br><span class="line">   *   1984  04    0.450090        0.483521</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * @param numRows Number of rows to show</span><br><span class="line">   *</span><br><span class="line">   * @group action</span><br><span class="line">   * @since 1.6.0</span><br><span class="line">   */</span><br><span class="line">显示datafram中的指定数量的数据，默认字段长度超过20位则截断。</span><br><span class="line">  def show(numRows: Int): Unit = show(numRows, truncate = true)</span><br><span class="line"></span><br><span class="line">显示datafram的数据，默认取前面20条记录显示，默认字段长度超过20位则截断。</span><br><span class="line">  </span><br><span class="line">  def show(): Unit = show(20)</span><br><span class="line"></span><br><span class="line">显示datafram的数据，默认取前面20条记录显示，通过truncate选择是否需要全部显示每一列的信息。</span><br><span class="line">  def show(truncate: Boolean): Unit = show(20, truncate)</span><br><span class="line"></span><br><span class="line">显示datafram的数据，numRows为显示数量，通过truncate选择是否需要全部显示每一列的信息。</span><br><span class="line">  def show(numRows: Int, truncate: Boolean): Unit = if (truncate) &#123;</span><br><span class="line">  def show(numRows: Int, truncate: Int): Unit = println(showString(numRows, truncate))</span><br></pre></td></tr></table></figure>
<h2 id="数据的关联"><a href="#数据的关联" class="headerlink" title="数据的关联"></a>数据的关联</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">返回dataset中空值操作算子</span><br><span class="line">  def na: DataFrameNaFunctions = new DataFrameNaFunctions(toDF())</span><br><span class="line"></span><br><span class="line">返回dataset中统计操作算子</span><br><span class="line">  def stat: DataFrameStatFunctions = new DataFrameStatFunctions(toDF())</span><br><span class="line">关联dataframe</span><br><span class="line">  def join(right: Dataset[_]): DataFrame = withPlan &#123;</span><br><span class="line">    Join(logicalPlan, right.logicalPlan, joinType = Inner, None)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Inner equi-join with another `DataFrame` using the given column.</span><br><span class="line">   *</span><br><span class="line">   * Different from other join functions, the join column will only appear once in the output,</span><br><span class="line">   * i.e. similar to SQL&apos;s `JOIN USING` syntax.</span><br><span class="line">   *</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   // Joining df1 and df2 using the column &quot;user_id&quot;</span><br><span class="line">   *   df1.join(df2, &quot;user_id&quot;)</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * @param right Right side of the join operation.</span><br><span class="line">   * @param usingColumn Name of the column to join on. This column must exist on both sides.</span><br><span class="line">   *</span><br><span class="line">   * @note If you perform a self-join using this function without aliasing the input</span><br><span class="line">   * `DataFrame`s, you will NOT be able to reference any columns after the join, since</span><br><span class="line">   * there is no way to disambiguate which side of the join you would like to reference.</span><br><span class="line">   *</span><br><span class="line">   * @group untypedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">指定字段关联</span><br><span class="line">  def join(right: Dataset[_], usingColumn: String): DataFrame = &#123;</span><br><span class="line">    join(right, Seq(usingColumn))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Inner equi-join with another `DataFrame` using the given columns.</span><br><span class="line">   *</span><br><span class="line">   * Different from other join functions, the join columns will only appear once in the output,</span><br><span class="line">   * i.e. similar to SQL&apos;s `JOIN USING` syntax.</span><br><span class="line">   *</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   // Joining df1 and df2 using the columns &quot;user_id&quot; and &quot;user_name&quot;</span><br><span class="line">   *   df1.join(df2, Seq(&quot;user_id&quot;, &quot;user_name&quot;))</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * @param right Right side of the join operation.</span><br><span class="line">   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.</span><br><span class="line">   *</span><br><span class="line">   * @note If you perform a self-join using this function without aliasing the input</span><br><span class="line">   * `DataFrame`s, you will NOT be able to reference any columns after the join, since</span><br><span class="line">   * there is no way to disambiguate which side of the join you would like to reference.</span><br><span class="line">   *</span><br><span class="line">   * @group untypedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">指定关联字段，不同dataframe中同名字段的key不重复出现</span><br><span class="line">  def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame = &#123;</span><br><span class="line">    join(right, usingColumns, &quot;inner&quot;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Equi-join with another `DataFrame` using the given columns. A cross join with a predicate</span><br><span class="line">   * is specified as an inner join. If you would explicitly like to perform a cross join use the</span><br><span class="line">   * `crossJoin` method.</span><br><span class="line">   *</span><br><span class="line">   * Different from other join functions, the join columns will only appear once in the output,</span><br><span class="line">   * i.e. similar to SQL&apos;s `JOIN USING` syntax.</span><br><span class="line">   *</span><br><span class="line">   * @param right Right side of the join operation.</span><br><span class="line">   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.</span><br><span class="line">   * @param joinType Type of join to perform. Default `inner`. Must be one of:</span><br><span class="line">   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,</span><br><span class="line">   *                 `right`, `right_outer`, `left_semi`, `left_anti`.</span><br><span class="line">   *</span><br><span class="line">   * @note If you perform a self-join using this function without aliasing the input</span><br><span class="line">   * `DataFrame`s, you will NOT be able to reference any columns after the join, since</span><br><span class="line">   * there is no way to disambiguate which side of the join you would like to reference.</span><br><span class="line">   *</span><br><span class="line">   * @group untypedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">指定关联的类型</span><br><span class="line">  def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame = &#123;</span><br><span class="line">    // Analyze the self join. The assumption is that the analyzer will disambiguate left vs right</span><br><span class="line">    // by creating a new instance for one of the branch.</span><br><span class="line">    val joined = sparkSession.sessionState.executePlan(</span><br><span class="line">      Join(logicalPlan, right.logicalPlan, joinType = JoinType(joinType), None))</span><br><span class="line">      .analyzed.asInstanceOf[Join]</span><br><span class="line"></span><br><span class="line">    withPlan &#123;</span><br><span class="line">      Join(</span><br><span class="line">        joined.left,</span><br><span class="line">        joined.right,</span><br><span class="line">        UsingJoin(JoinType(joinType), usingColumns),</span><br><span class="line">        None)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Inner join with another `DataFrame`, using the given join expression.</span><br><span class="line">   *</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   // The following two are equivalent:</span><br><span class="line">   *   df1.join(df2, $&quot;df1Key&quot; === $&quot;df2Key&quot;)</span><br><span class="line">   *   df1.join(df2).where($&quot;df1Key&quot; === $&quot;df2Key&quot;)</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * @group untypedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">通过表达式进行关联</span><br><span class="line">  def join(right: Dataset[_], joinExprs: Column): DataFrame = join(right, joinExprs, &quot;inner&quot;)</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Join with another `DataFrame`, using the given join expression. The following performs</span><br><span class="line">   * a full outer join between `df1` and `df2`.</span><br><span class="line">   *</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   // Scala:</span><br><span class="line">   *   import org.apache.spark.sql.functions._</span><br><span class="line">   *   df1.join(df2, $&quot;df1Key&quot; === $&quot;df2Key&quot;, &quot;outer&quot;)</span><br><span class="line">   *</span><br><span class="line">   *   // Java:</span><br><span class="line">   *   import static org.apache.spark.sql.functions.*;</span><br><span class="line">   *   df1.join(df2, col(&quot;df1Key&quot;).equalTo(col(&quot;df2Key&quot;)), &quot;outer&quot;);</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * @param right Right side of the join.</span><br><span class="line">   * @param joinExprs Join expression.</span><br><span class="line">   * @param joinType Type of join to perform. Default `inner`. Must be one of:</span><br><span class="line">   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,</span><br><span class="line">   *                 `right`, `right_outer`, `left_semi`, `left_anti`.</span><br><span class="line">   *</span><br><span class="line">   * @group untypedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">通过列名表达式进行关联</span><br><span class="line">  def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame = &#123;</span><br><span class="line">    // Note that in this function, we introduce a hack in the case of self-join to automatically</span><br><span class="line">    // resolve ambiguous join conditions into ones that might make sense [SPARK-6231].</span><br><span class="line">    // Consider this case: df.join(df, df(&quot;key&quot;) === df(&quot;key&quot;))</span><br><span class="line">    // Since df(&quot;key&quot;) === df(&quot;key&quot;) is a trivially true condition, this actually becomes a</span><br><span class="line">    // cartesian join. However, most likely users expect to perform a self join using &quot;key&quot;.</span><br><span class="line">    // With that assumption, this hack turns the trivially true condition into equality on join</span><br><span class="line">    // keys that are resolved to both sides.</span><br><span class="line"></span><br><span class="line">    // Trigger analysis so in the case of self-join, the analyzer will clone the plan.</span><br><span class="line">    // After the cloning, left and right side will have distinct expression ids.</span><br><span class="line">    val plan = withPlan(</span><br><span class="line">      Join(logicalPlan, right.logicalPlan, JoinType(joinType), Some(joinExprs.expr)))</span><br><span class="line">      .queryExecution.analyzed.asInstanceOf[Join]</span><br><span class="line"></span><br><span class="line">    // If auto self join alias is disabled, return the plan.</span><br><span class="line">    if (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) &#123;</span><br><span class="line">      return withPlan(plan)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // If left/right have no output set intersection, return the plan.</span><br><span class="line">    val lanalyzed = withPlan(this.logicalPlan).queryExecution.analyzed</span><br><span class="line">    val ranalyzed = withPlan(right.logicalPlan).queryExecution.analyzed</span><br><span class="line">    if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) &#123;</span><br><span class="line">      return withPlan(plan)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // Otherwise, find the trivially true predicates and automatically resolves them to both sides.</span><br><span class="line">    // By the time we get here, since we have already run analysis, all attributes should&apos;ve been</span><br><span class="line">    // resolved and become AttributeReference.</span><br><span class="line">    val cond = plan.condition.map &#123; _.transform &#123;</span><br><span class="line">      case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference)</span><br><span class="line">          if a.sameRef(b) =&gt;</span><br><span class="line">        catalyst.expressions.EqualTo(</span><br><span class="line">          withPlan(plan.left).resolve(a.name),</span><br><span class="line">          withPlan(plan.right).resolve(b.name))</span><br><span class="line">    &#125;&#125;</span><br><span class="line"></span><br><span class="line">    withPlan &#123;</span><br><span class="line">      plan.copy(condition = cond)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Explicit cartesian join with another `DataFrame`.</span><br><span class="line">   *</span><br><span class="line">   * @param right Right side of the join operation.</span><br><span class="line">   *</span><br><span class="line">   * @note Cartesian joins are very expensive without an extra filter that can be pushed down.</span><br><span class="line">   *</span><br><span class="line">   * @group untypedrel</span><br><span class="line">   * @since 2.1.0</span><br><span class="line">   */</span><br><span class="line">全表关联</span><br><span class="line">  def crossJoin(right: Dataset[_]): DataFrame = withPlan &#123;</span><br><span class="line">    Join(logicalPlan, right.logicalPlan, joinType = Cross, None)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * :: Experimental ::</span><br><span class="line">   * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to</span><br><span class="line">   * true.</span><br><span class="line">   *</span><br><span class="line">   * This is similar to the relation `join` function with one important difference in the</span><br><span class="line">   * result schema. Since `joinWith` preserves objects present on either side of the join, the</span><br><span class="line">   * result schema is similarly nested into a tuple under the column names `_1` and `_2`.</span><br><span class="line">   *</span><br><span class="line">   * This type of join can be useful both for preserving type-safety with the original object</span><br><span class="line">   * types as well as working with relational data where either side of the join has column</span><br><span class="line">   * names in common.</span><br><span class="line">   *</span><br><span class="line">   * @param other Right side of the join.</span><br><span class="line">   * @param condition Join expression.</span><br><span class="line">   * @param joinType Type of join to perform. Default `inner`. Must be one of:</span><br><span class="line">   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,</span><br><span class="line">   *                 `right`, `right_outer`, `left_semi`, `left_anti`.</span><br><span class="line">   *</span><br><span class="line">   * @group typedrel</span><br><span class="line">   * @since 1.6.0</span><br><span class="line">   */</span><br><span class="line">一种特殊的关联，得到的结果集的结构不同于普通的关联结果</span><br><span class="line">  @Experimental</span><br><span class="line">  @InterfaceStability.Evolving</span><br><span class="line">  def joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = &#123;</span><br><span class="line">    // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved,</span><br><span class="line">    // etc.</span><br><span class="line">    val joined = sparkSession.sessionState.executePlan(</span><br><span class="line">      Join(</span><br><span class="line">        this.logicalPlan,</span><br><span class="line">        other.logicalPlan,</span><br><span class="line">        JoinType(joinType),</span><br><span class="line">        Some(condition.expr))).analyzed.asInstanceOf[Join]</span><br><span class="line"></span><br><span class="line">    // For both join side, combine all outputs into a single column and alias it with &quot;_1&quot; or &quot;_2&quot;,</span><br><span class="line">    // to match the schema for the encoder of the join result.</span><br><span class="line">    // Note that we do this before joining them, to enable the join operator to return null for one</span><br><span class="line">    // side, in cases like outer-join.</span><br><span class="line">    val left = &#123;</span><br><span class="line">      val combined = if (this.exprEnc.flat) &#123;</span><br><span class="line">        assert(joined.left.output.length == 1)</span><br><span class="line">        Alias(joined.left.output.head, &quot;_1&quot;)()</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        Alias(CreateStruct(joined.left.output), &quot;_1&quot;)()</span><br><span class="line">      &#125;</span><br><span class="line">      Project(combined :: Nil, joined.left)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    val right = &#123;</span><br><span class="line">      val combined = if (other.exprEnc.flat) &#123;</span><br><span class="line">        assert(joined.right.output.length == 1)</span><br><span class="line">        Alias(joined.right.output.head, &quot;_2&quot;)()</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        Alias(CreateStruct(joined.right.output), &quot;_2&quot;)()</span><br><span class="line">      &#125;</span><br><span class="line">      Project(combined :: Nil, joined.right)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // Rewrites the join condition to make the attribute point to correct column/field, after we</span><br><span class="line">    // combine the outputs of each join side.</span><br><span class="line">    val conditionExpr = joined.condition.get transformUp &#123;</span><br><span class="line">      case a: Attribute if joined.left.outputSet.contains(a) =&gt;</span><br><span class="line">        if (this.exprEnc.flat) &#123;</span><br><span class="line">          left.output.head</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          val index = joined.left.output.indexWhere(_.exprId == a.exprId)</span><br><span class="line">          GetStructField(left.output.head, index)</span><br><span class="line">        &#125;</span><br><span class="line">      case a: Attribute if joined.right.outputSet.contains(a) =&gt;</span><br><span class="line">        if (other.exprEnc.flat) &#123;</span><br><span class="line">          right.output.head</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          val index = joined.right.output.indexWhere(_.exprId == a.exprId)</span><br><span class="line">          GetStructField(right.output.head, index)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    implicit val tuple2Encoder: Encoder[(T, U)] =</span><br><span class="line">      ExpressionEncoder.tuple(this.exprEnc, other.exprEnc)</span><br><span class="line"></span><br><span class="line">    withTypedPlan(Join(left, right, joined.joinType, Some(conditionExpr)))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * :: Experimental ::</span><br><span class="line">   * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair</span><br><span class="line">   * where `condition` evaluates to true.</span><br><span class="line">   *</span><br><span class="line">   * @param other Right side of the join.</span><br><span class="line">   * @param condition Join expression.</span><br><span class="line">   *</span><br><span class="line">   * @group typedrel</span><br><span class="line">   * @since 1.6.0</span><br><span class="line">   */</span><br><span class="line">  @Experimental</span><br><span class="line">  @InterfaceStability.Evolving</span><br><span class="line">  def joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = &#123;</span><br><span class="line">    joinWith(other, condition, &quot;inner&quot;)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h2 id="排序分组"><a href="#排序分组" class="headerlink" title="排序分组"></a>排序分组</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">  /**</span><br><span class="line">   * Returns a new Dataset with each partition sorted by the given expressions.</span><br><span class="line">   *</span><br><span class="line">   * This is the same operation as &quot;SORT BY&quot; in SQL (Hive QL).</span><br><span class="line">   *</span><br><span class="line">   * @group typedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">根据指定字段对每个分区进行排序</span><br><span class="line">  @scala.annotation.varargs</span><br><span class="line">  def sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T] = &#123;</span><br><span class="line">    sortWithinPartitions((sortCol +: sortCols).map(Column(_)) : _*)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Returns a new Dataset with each partition sorted by the given expressions.</span><br><span class="line">   *</span><br><span class="line">   * This is the same operation as &quot;SORT BY&quot; in SQL (Hive QL).</span><br><span class="line">   *</span><br><span class="line">   * @group typedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line"></span><br><span class="line">根据指定列对每个分区进行排序</span><br><span class="line">  @scala.annotation.varargs</span><br><span class="line">  def sortWithinPartitions(sortExprs: Column*): Dataset[T] = &#123;</span><br><span class="line">    sortInternal(global = false, sortExprs)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Returns a new Dataset sorted by the specified column, all in ascending order.</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   // The following 3 are equivalent</span><br><span class="line">   *   ds.sort(&quot;sortcol&quot;)</span><br><span class="line">   *   ds.sort($&quot;sortcol&quot;)</span><br><span class="line">   *   ds.sort($&quot;sortcol&quot;.asc)</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * @group typedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  @scala.annotation.varargs</span><br><span class="line">  def sort(sortCol: String, sortCols: String*): Dataset[T] = &#123;</span><br><span class="line">    sort((sortCol +: sortCols).map(apply) : _*)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Returns a new Dataset sorted by the given expressions. For example:</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   ds.sort($&quot;col1&quot;, $&quot;col2&quot;.desc)</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * @group typedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  @scala.annotation.varargs</span><br><span class="line">  def sort(sortExprs: Column*): Dataset[T] = &#123;</span><br><span class="line">    sortInternal(global = true, sortExprs)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Returns a new Dataset sorted by the given expressions.</span><br><span class="line">   * This is an alias of the `sort` function.</span><br><span class="line">   *</span><br><span class="line">   * @group typedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  @scala.annotation.varargs</span><br><span class="line">  def orderBy(sortCol: String, sortCols: String*): Dataset[T] = sort(sortCol, sortCols : _*)</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Returns a new Dataset sorted by the given expressions.</span><br><span class="line">   * This is an alias of the `sort` function.</span><br><span class="line">   *</span><br><span class="line">   * @group typedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  @scala.annotation.varargs</span><br><span class="line">  def orderBy(sortExprs: Column*): Dataset[T] = sort(sortExprs : _*)</span><br></pre></td></tr></table></figure>
<p>提取指定的列<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Selects column based on the column name and return it as a [[Column]].</span><br><span class="line"> *</span><br><span class="line"> * @note The column name can also reference to a nested column like `a.b`.</span><br><span class="line"> *</span><br><span class="line"> * @group untypedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">def apply(colName: String): Column = col(colName)</span><br><span class="line">/**</span><br><span class="line"> * Selects column based on the column name and return it as a [[Column]].</span><br><span class="line"> *</span><br><span class="line"> * @note The column name can also reference to a nested column like `a.b`.</span><br><span class="line"> *</span><br><span class="line"> * @group untypedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">def col(colName: String): Column = colName match &#123;</span><br><span class="line">  case &quot;*&quot; =&gt;</span><br><span class="line">    Column(ResolvedStar(queryExecution.analyzed.output))</span><br><span class="line">  case _ =&gt;</span><br><span class="line">    val expr = resolve(colName)</span><br><span class="line">    Column(expr)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="别名"><a href="#别名" class="headerlink" title="别名"></a>别名</h2><p>别名有给列取别名的也有给dataset取别名的这里是给当前dataset取别名<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Returns a new Dataset with an alias set.</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">def as(alias: String): Dataset[T] = withTypedPlan &#123;</span><br><span class="line">  SubqueryAlias(alias, logicalPlan, None)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * (Scala-specific) Returns a new Dataset with an alias set.</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">def as(alias: Symbol): Dataset[T] = as(alias.name)</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Returns a new Dataset with an alias set. Same as `as`.</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">def alias(alias: String): Dataset[T] = as(alias)</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`.</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">def alias(alias: Symbol): Dataset[T] = as(alias)</span><br></pre></td></tr></table></figure></p>
<h2 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h2><p>查询有很多种接口使用的方式不太一样<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br></pre></td><td class="code"><pre><span class="line">  /**</span><br><span class="line">   * Selects a set of column based expressions.</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   ds.select($&quot;colA&quot;, $&quot;colB&quot; + 1)</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * @group untypedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  @scala.annotation.varargs</span><br><span class="line">  def select(cols: Column*): DataFrame = withPlan &#123;</span><br><span class="line">    Project(cols.map(_.named), logicalPlan)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Selects a set of columns. This is a variant of `select` that can only select</span><br><span class="line">   * existing columns using column names (i.e. cannot construct expressions).</span><br><span class="line">   *</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   // The following two are equivalent:</span><br><span class="line">   *   ds.select(&quot;colA&quot;, &quot;colB&quot;)</span><br><span class="line">   *   ds.select($&quot;colA&quot;, $&quot;colB&quot;)</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * @group untypedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  @scala.annotation.varargs</span><br><span class="line">  def select(col: String, cols: String*): DataFrame = select((col +: cols).map(Column(_)) : _*)</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Selects a set of SQL expressions. This is a variant of `select` that accepts</span><br><span class="line">   * SQL expressions.</span><br><span class="line">   *</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   // The following are equivalent:</span><br><span class="line">   *   ds.selectExpr(&quot;colA&quot;, &quot;colB as newName&quot;, &quot;abs(colC)&quot;)</span><br><span class="line">   *   ds.select(expr(&quot;colA&quot;), expr(&quot;colB as newName&quot;), expr(&quot;abs(colC)&quot;))</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * @group untypedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">按照表达式来查询</span><br><span class="line">  @scala.annotation.varargs</span><br><span class="line">  def selectExpr(exprs: String*): DataFrame = &#123;</span><br><span class="line">    select(exprs.map &#123; expr =&gt;</span><br><span class="line">      Column(sparkSession.sessionState.sqlParser.parseExpression(expr))</span><br><span class="line">    &#125;: _*)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * :: Experimental ::</span><br><span class="line">   * Returns a new Dataset by computing the given [[Column]] expression for each element.</span><br><span class="line">   *</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   val ds = Seq(1, 2, 3).toDS()</span><br><span class="line">   *   val newDS = ds.select(expr(&quot;value + 1&quot;).as[Int])</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * @group typedrel</span><br><span class="line">   * @since 1.6.0</span><br><span class="line">   */</span><br><span class="line">  @Experimental</span><br><span class="line">  @InterfaceStability.Evolving</span><br><span class="line">  def select[U1](c1: TypedColumn[T, U1]): Dataset[U1] = &#123;</span><br><span class="line">    implicit val encoder = c1.encoder</span><br><span class="line">    val project = Project(c1.withInputType(exprEnc, logicalPlan.output).named :: Nil,</span><br><span class="line">      logicalPlan)</span><br><span class="line"></span><br><span class="line">    if (encoder.flat) &#123;</span><br><span class="line">      new Dataset[U1](sparkSession, project, encoder)</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      // Flattens inner fields of U1</span><br><span class="line">      new Dataset[Tuple1[U1]](sparkSession, project, ExpressionEncoder.tuple(encoder)).map(_._1)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Internal helper function for building typed selects that return tuples. For simplicity and</span><br><span class="line">   * code reuse, we do this without the help of the type system and then use helper functions</span><br><span class="line">   * that cast appropriately for the user facing interface.</span><br><span class="line">   */</span><br><span class="line">  ???这个查询怎么用</span><br><span class="line">  protected def selectUntyped(columns: TypedColumn[_, _]*): Dataset[_] = &#123;</span><br><span class="line">    val encoders = columns.map(_.encoder)</span><br><span class="line">    val namedColumns =</span><br><span class="line">      columns.map(_.withInputType(exprEnc, logicalPlan.output).named)</span><br><span class="line">    val execution = new QueryExecution(sparkSession, Project(namedColumns, logicalPlan))</span><br><span class="line">    new Dataset(sparkSession, execution, ExpressionEncoder.tuple(encoders))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * :: Experimental ::</span><br><span class="line">   * Returns a new Dataset by computing the given [[Column]] expressions for each element.</span><br><span class="line">   *</span><br><span class="line">   * @group typedrel</span><br><span class="line">   * @since 1.6.0</span><br><span class="line">   */</span><br><span class="line">  @Experimental</span><br><span class="line">  @InterfaceStability.Evolving</span><br><span class="line">  def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)] =</span><br><span class="line">    selectUntyped(c1, c2).asInstanceOf[Dataset[(U1, U2)]]</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * :: Experimental ::</span><br><span class="line">   * Returns a new Dataset by computing the given [[Column]] expressions for each element.</span><br><span class="line">   *</span><br><span class="line">   * @group typedrel</span><br><span class="line">   * @since 1.6.0</span><br><span class="line">   */</span><br><span class="line">  @Experimental</span><br><span class="line">  @InterfaceStability.Evolving</span><br><span class="line">  def select[U1, U2, U3](</span><br><span class="line">      c1: TypedColumn[T, U1],</span><br><span class="line">      c2: TypedColumn[T, U2],</span><br><span class="line">      c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)] =</span><br><span class="line">    selectUntyped(c1, c2, c3).asInstanceOf[Dataset[(U1, U2, U3)]]</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * :: Experimental ::</span><br><span class="line">   * Returns a new Dataset by computing the given [[Column]] expressions for each element.</span><br><span class="line">   *</span><br><span class="line">   * @group typedrel</span><br><span class="line">   * @since 1.6.0</span><br><span class="line">   */</span><br><span class="line">  @Experimental</span><br><span class="line">  @InterfaceStability.Evolving</span><br><span class="line">  def select[U1, U2, U3, U4](</span><br><span class="line">      c1: TypedColumn[T, U1],</span><br><span class="line">      c2: TypedColumn[T, U2],</span><br><span class="line">      c3: TypedColumn[T, U3],</span><br><span class="line">      c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)] =</span><br><span class="line">    selectUntyped(c1, c2, c3, c4).asInstanceOf[Dataset[(U1, U2, U3, U4)]]</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * :: Experimental ::</span><br><span class="line">   * Returns a new Dataset by computing the given [[Column]] expressions for each element.</span><br><span class="line">   *</span><br><span class="line">   * @group typedrel</span><br><span class="line">   * @since 1.6.0</span><br><span class="line">   */</span><br><span class="line">  @Experimental</span><br><span class="line">  @InterfaceStability.Evolving</span><br><span class="line">  def select[U1, U2, U3, U4, U5](</span><br><span class="line">      c1: TypedColumn[T, U1],</span><br><span class="line">      c2: TypedColumn[T, U2],</span><br><span class="line">      c3: TypedColumn[T, U3],</span><br><span class="line">      c4: TypedColumn[T, U4],</span><br><span class="line">      c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)] =</span><br><span class="line">    selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[Dataset[(U1, U2, U3, U4, U5)]]</span><br></pre></td></tr></table></figure></p>
<h2 id="过滤"><a href="#过滤" class="headerlink" title="过滤"></a>过滤</h2><p>过滤，这里的过滤和sql里面的where条件是相同的，查询满足一定条件的记录。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Filters rows using the given condition.</span><br><span class="line"> * &#123;&#123;&#123;</span><br><span class="line"> *   // The following are equivalent:</span><br><span class="line"> *   peopleDs.filter($&quot;age&quot; &gt; 15)</span><br><span class="line"> *   peopleDs.where($&quot;age&quot; &gt; 15)</span><br><span class="line"> * &#125;&#125;&#125;</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">def filter(condition: Column): Dataset[T] = withTypedPlan &#123;</span><br><span class="line">  Filter(condition.expr, logicalPlan)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Filters rows using the given SQL expression.</span><br><span class="line"> * &#123;&#123;&#123;</span><br><span class="line"> *   peopleDs.filter(&quot;age &gt; 15&quot;)</span><br><span class="line"> * &#125;&#125;&#125;</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">def filter(conditionExpr: String): Dataset[T] = &#123;</span><br><span class="line">  filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Filters rows using the given condition. This is an alias for `filter`.</span><br><span class="line"> * &#123;&#123;&#123;</span><br><span class="line"> *   // The following are equivalent:</span><br><span class="line"> *   peopleDs.filter($&quot;age&quot; &gt; 15)</span><br><span class="line"> *   peopleDs.where($&quot;age&quot; &gt; 15)</span><br><span class="line"> * &#125;&#125;&#125;</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">def where(condition: Column): Dataset[T] = filter(condition)</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Filters rows using the given SQL expression.</span><br><span class="line"> * &#123;&#123;&#123;</span><br><span class="line"> *   peopleDs.where(&quot;age &gt; 15&quot;)</span><br><span class="line"> * &#125;&#125;&#125;</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">def where(conditionExpr: String): Dataset[T] = &#123;</span><br><span class="line">  filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="分组查询"><a href="#分组查询" class="headerlink" title="分组查询"></a>分组查询</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Groups the Dataset using the specified columns, so we can run aggregation on them. See</span><br><span class="line"> * [[RelationalGroupedDataset]] for all the available aggregate functions.</span><br><span class="line"> *</span><br><span class="line"> * &#123;&#123;&#123;</span><br><span class="line"> *   // Compute the average for all numeric columns grouped by department.</span><br><span class="line"> *   ds.groupBy($&quot;department&quot;).avg()</span><br><span class="line"> *</span><br><span class="line"> *   // Compute the max age and average salary, grouped by department and gender.</span><br><span class="line"> *   ds.groupBy($&quot;department&quot;, $&quot;gender&quot;).agg(Map(</span><br><span class="line"> *     &quot;salary&quot; -&gt; &quot;avg&quot;,</span><br><span class="line"> *     &quot;age&quot; -&gt; &quot;max&quot;</span><br><span class="line"> *   ))</span><br><span class="line"> * &#125;&#125;&#125;</span><br><span class="line"> *</span><br><span class="line"> * @group untypedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">@scala.annotation.varargs</span><br><span class="line">def groupBy(cols: Column*): RelationalGroupedDataset = &#123;</span><br><span class="line">  RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.GroupByType)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Create a multi-dimensional rollup for the current Dataset using the specified columns,</span><br><span class="line"> * so we can run aggregation on them.</span><br><span class="line"> * See [[RelationalGroupedDataset]] for all the available aggregate functions.</span><br><span class="line"> *</span><br><span class="line"> * &#123;&#123;&#123;</span><br><span class="line"> *   // Compute the average for all numeric columns rolluped by department and group.</span><br><span class="line"> *   ds.rollup($&quot;department&quot;, $&quot;group&quot;).avg()</span><br><span class="line"> *</span><br><span class="line"> *   // Compute the max age and average salary, rolluped by department and gender.</span><br><span class="line"> *   ds.rollup($&quot;department&quot;, $&quot;gender&quot;).agg(Map(</span><br><span class="line"> *     &quot;salary&quot; -&gt; &quot;avg&quot;,</span><br><span class="line"> *     &quot;age&quot; -&gt; &quot;max&quot;</span><br><span class="line"> *   ))</span><br><span class="line"> * &#125;&#125;&#125;</span><br><span class="line"> *</span><br><span class="line"> * @group untypedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">数据归纳</span><br><span class="line">@scala.annotation.varargs</span><br><span class="line">def rollup(cols: Column*): RelationalGroupedDataset = &#123;</span><br><span class="line">  RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.RollupType)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Create a multi-dimensional cube for the current Dataset using the specified columns,</span><br><span class="line"> * so we can run aggregation on them.</span><br><span class="line"> * See [[RelationalGroupedDataset]] for all the available aggregate functions.</span><br><span class="line"> *</span><br><span class="line"> * &#123;&#123;&#123;</span><br><span class="line"> *   // Compute the average for all numeric columns cubed by department and group.</span><br><span class="line"> *   ds.cube($&quot;department&quot;, $&quot;group&quot;).avg()</span><br><span class="line"> *</span><br><span class="line"> *   // Compute the max age and average salary, cubed by department and gender.</span><br><span class="line"> *   ds.cube($&quot;department&quot;, $&quot;gender&quot;).agg(Map(</span><br><span class="line"> *     &quot;salary&quot; -&gt; &quot;avg&quot;,</span><br><span class="line"> *     &quot;age&quot; -&gt; &quot;max&quot;</span><br><span class="line"> *   ))</span><br><span class="line"> * &#125;&#125;&#125;</span><br><span class="line"> *</span><br><span class="line"> * @group untypedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">@scala.annotation.varargs</span><br><span class="line">def cube(cols: Column*): RelationalGroupedDataset = &#123;</span><br><span class="line">  RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.CubeType)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Groups the Dataset using the specified columns, so that we can run aggregation on them.</span><br><span class="line"> * See [[RelationalGroupedDataset]] for all the available aggregate functions.</span><br><span class="line"> *</span><br><span class="line"> * This is a variant of groupBy that can only group by existing columns using column names</span><br><span class="line"> * (i.e. cannot construct expressions).</span><br><span class="line"> *</span><br><span class="line"> * &#123;&#123;&#123;</span><br><span class="line"> *   // Compute the average for all numeric columns grouped by department.</span><br><span class="line"> *   ds.groupBy(&quot;department&quot;).avg()</span><br><span class="line"> *</span><br><span class="line"> *   // Compute the max age and average salary, grouped by department and gender.</span><br><span class="line"> *   ds.groupBy($&quot;department&quot;, $&quot;gender&quot;).agg(Map(</span><br><span class="line"> *     &quot;salary&quot; -&gt; &quot;avg&quot;,</span><br><span class="line"> *     &quot;age&quot; -&gt; &quot;max&quot;</span><br><span class="line"> *   ))</span><br><span class="line"> * &#125;&#125;&#125;</span><br><span class="line"> * @group untypedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">@scala.annotation.varargs</span><br><span class="line">def groupBy(col1: String, cols: String*): RelationalGroupedDataset = &#123;</span><br><span class="line">  val colNames: Seq[String] = col1 +: cols</span><br><span class="line">  RelationalGroupedDataset(</span><br><span class="line">    toDF(), colNames.map(colName =&gt; resolve(colName)), RelationalGroupedDataset.GroupByType)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="reduce操作"><a href="#reduce操作" class="headerlink" title="reduce操作"></a>reduce操作</h2>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * :: Experimental ::</span><br><span class="line"> * (Scala-specific)</span><br><span class="line"> * Reduces the elements of this Dataset using the specified binary function. The given `func`</span><br><span class="line"> * must be commutative and associative or the result may be non-deterministic.</span><br><span class="line"> *</span><br><span class="line"> * @group action</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">合并操作</span><br><span class="line">@Experimental</span><br><span class="line">@InterfaceStability.Evolving</span><br><span class="line">def reduce(func: (T, T) =&gt; T): T = rdd.reduce(func)</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * :: Experimental ::</span><br><span class="line"> * (Java-specific)</span><br><span class="line"> * Reduces the elements of this Dataset using the specified binary function. The given `func`</span><br><span class="line"> * must be commutative and associative or the result may be non-deterministic.</span><br><span class="line"> *</span><br><span class="line"> * @group action</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">@Experimental</span><br><span class="line">@InterfaceStability.Evolving</span><br><span class="line">def reduce(func: ReduceFunction[T]): T = reduce(func.call(_, _))</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * :: Experimental ::</span><br><span class="line"> * (Scala-specific)</span><br><span class="line"> * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">分组                                  </span><br><span class="line">@Experimental</span><br><span class="line">@InterfaceStability.Evolving</span><br><span class="line">def groupByKey[K: Encoder](func: T =&gt; K): KeyValueGroupedDataset[K, T] = &#123;</span><br><span class="line">  val inputPlan = logicalPlan</span><br><span class="line">  val withGroupingKey = AppendColumns(func, inputPlan)</span><br><span class="line">  val executed = sparkSession.sessionState.executePlan(withGroupingKey)</span><br><span class="line"></span><br><span class="line">  new KeyValueGroupedDataset(</span><br><span class="line">    encoderFor[K],</span><br><span class="line">    encoderFor[T],</span><br><span class="line">    executed,</span><br><span class="line">    inputPlan.output,</span><br><span class="line">    withGroupingKey.newColumns)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * :: Experimental ::</span><br><span class="line"> * (Java-specific)</span><br><span class="line"> * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">@Experimental</span><br><span class="line">@InterfaceStability.Evolving</span><br><span class="line">def groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T] =</span><br><span class="line">  groupByKey(func.call(_))(encoder)</span><br></pre></td></tr></table></figure>
<h2 id="数据钻取与聚合操作"><a href="#数据钻取与聚合操作" class="headerlink" title="数据钻取与聚合操作"></a>数据钻取与聚合操作</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Create a multi-dimensional rollup for the current Dataset using the specified columns,</span><br><span class="line"> * so we can run aggregation on them.</span><br><span class="line"> * See [[RelationalGroupedDataset]] for all the available aggregate functions.</span><br><span class="line"> *</span><br><span class="line"> * This is a variant of rollup that can only group by existing columns using column names</span><br><span class="line"> * (i.e. cannot construct expressions).</span><br><span class="line"> *</span><br><span class="line"> * &#123;&#123;&#123;</span><br><span class="line"> *   // Compute the average for all numeric columns rolluped by department and group.</span><br><span class="line"> *   ds.rollup(&quot;department&quot;, &quot;group&quot;).avg()</span><br><span class="line"> *</span><br><span class="line"> *   // Compute the max age and average salary, rolluped by department and gender.</span><br><span class="line"> *   ds.rollup($&quot;department&quot;, $&quot;gender&quot;).agg(Map(</span><br><span class="line"> *     &quot;salary&quot; -&gt; &quot;avg&quot;,</span><br><span class="line"> *     &quot;age&quot; -&gt; &quot;max&quot;</span><br><span class="line"> *   ))</span><br><span class="line"> * &#125;&#125;&#125;</span><br><span class="line"> *</span><br><span class="line"> * @group untypedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">@scala.annotation.varargs</span><br><span class="line">def rollup(col1: String, cols: String*): RelationalGroupedDataset = &#123;</span><br><span class="line">  val colNames: Seq[String] = col1 +: cols</span><br><span class="line">  RelationalGroupedDataset(</span><br><span class="line">    toDF(), colNames.map(colName =&gt; resolve(colName)), RelationalGroupedDataset.RollupType)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Create a multi-dimensional cube for the current Dataset using the specified columns,</span><br><span class="line"> * so we can run aggregation on them.</span><br><span class="line"> * See [[RelationalGroupedDataset]] for all the available aggregate functions.</span><br><span class="line"> *</span><br><span class="line"> * This is a variant of cube that can only group by existing columns using column names</span><br><span class="line"> * (i.e. cannot construct expressions).</span><br><span class="line"> *</span><br><span class="line"> * &#123;&#123;&#123;</span><br><span class="line"> *   // Compute the average for all numeric columns cubed by department and group.</span><br><span class="line"> *   ds.cube(&quot;department&quot;, &quot;group&quot;).avg()</span><br><span class="line"> *</span><br><span class="line"> *   // Compute the max age and average salary, cubed by department and gender.</span><br><span class="line"> *   ds.cube($&quot;department&quot;, $&quot;gender&quot;).agg(Map(</span><br><span class="line"> *     &quot;salary&quot; -&gt; &quot;avg&quot;,</span><br><span class="line"> *     &quot;age&quot; -&gt; &quot;max&quot;</span><br><span class="line"> *   ))</span><br><span class="line"> * &#125;&#125;&#125;</span><br><span class="line"> * @group untypedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">@scala.annotation.varargs</span><br><span class="line">def cube(col1: String, cols: String*): RelationalGroupedDataset = &#123;</span><br><span class="line">  val colNames: Seq[String] = col1 +: cols</span><br><span class="line">  RelationalGroupedDataset(</span><br><span class="line">    toDF(), colNames.map(colName =&gt; resolve(colName)), RelationalGroupedDataset.CubeType)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * (Scala-specific) Aggregates on the entire Dataset without groups.</span><br><span class="line"> * &#123;&#123;&#123;</span><br><span class="line"> *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)</span><br><span class="line"> *   ds.agg(&quot;age&quot; -&gt; &quot;max&quot;, &quot;salary&quot; -&gt; &quot;avg&quot;)</span><br><span class="line"> *   ds.groupBy().agg(&quot;age&quot; -&gt; &quot;max&quot;, &quot;salary&quot; -&gt; &quot;avg&quot;)</span><br><span class="line"> * &#125;&#125;&#125;</span><br><span class="line"> *</span><br><span class="line"> * @group untypedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = &#123;</span><br><span class="line">  groupBy().agg(aggExpr, aggExprs : _*)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * (Scala-specific) Aggregates on the entire Dataset without groups.</span><br><span class="line"> * &#123;&#123;&#123;</span><br><span class="line"> *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)</span><br><span class="line"> *   ds.agg(Map(&quot;age&quot; -&gt; &quot;max&quot;, &quot;salary&quot; -&gt; &quot;avg&quot;))</span><br><span class="line"> *   ds.groupBy().agg(Map(&quot;age&quot; -&gt; &quot;max&quot;, &quot;salary&quot; -&gt; &quot;avg&quot;))</span><br><span class="line"> * &#125;&#125;&#125;</span><br><span class="line"> *</span><br><span class="line"> * @group untypedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">def agg(exprs: Map[String, String]): DataFrame = groupBy().agg(exprs)</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * (Java-specific) Aggregates on the entire Dataset without groups.</span><br><span class="line"> * &#123;&#123;&#123;</span><br><span class="line"> *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)</span><br><span class="line"> *   ds.agg(Map(&quot;age&quot; -&gt; &quot;max&quot;, &quot;salary&quot; -&gt; &quot;avg&quot;))</span><br><span class="line"> *   ds.groupBy().agg(Map(&quot;age&quot; -&gt; &quot;max&quot;, &quot;salary&quot; -&gt; &quot;avg&quot;))</span><br><span class="line"> * &#125;&#125;&#125;</span><br><span class="line"> *</span><br><span class="line"> * @group untypedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">def agg(exprs: java.util.Map[String, String]): DataFrame = groupBy().agg(exprs)</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Aggregates on the entire Dataset without groups.</span><br><span class="line"> * &#123;&#123;&#123;</span><br><span class="line"> *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)</span><br><span class="line"> *   ds.agg(max($&quot;age&quot;), avg($&quot;salary&quot;))</span><br><span class="line"> *   ds.groupBy().agg(max($&quot;age&quot;), avg($&quot;salary&quot;))</span><br><span class="line"> * &#125;&#125;&#125;</span><br><span class="line"> *</span><br><span class="line"> * @group untypedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">@scala.annotation.varargs</span><br><span class="line">def agg(expr: Column, exprs: Column*): DataFrame = groupBy().agg(expr, exprs : _*)</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Returns a new Dataset by taking the first `n` rows. The difference between this function</span><br><span class="line"> * and `head` is that `head` is an action and returns an array (by triggering query execution)</span><br><span class="line"> * while `limit` returns a new Dataset.</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">def limit(n: Int): Dataset[T] = withTypedPlan &#123;</span><br><span class="line">  Limit(Literal(n), logicalPlan)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="集合的交并补接口"><a href="#集合的交并补接口" class="headerlink" title="集合的交并补接口"></a>集合的交并补接口</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Returns a new Dataset containing union of rows in this Dataset and another Dataset.</span><br><span class="line"> * This is equivalent to `UNION ALL` in SQL.</span><br><span class="line"> *</span><br><span class="line"> * To do a SQL-style set union (that does deduplication of elements), use this function followed</span><br><span class="line"> * by a [[distinct]].</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">@deprecated(&quot;use union()&quot;, &quot;2.0.0&quot;)</span><br><span class="line">def unionAll(other: Dataset[T]): Dataset[T] = union(other)</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Returns a new Dataset containing union of rows in this Dataset and another Dataset.</span><br><span class="line"> * This is equivalent to `UNION ALL` in SQL.</span><br><span class="line"> *</span><br><span class="line"> * To do a SQL-style set union (that does deduplication of elements), use this function followed</span><br><span class="line"> * by a [[distinct]].</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">def union(other: Dataset[T]): Dataset[T] = withSetOperator &#123;</span><br><span class="line">  // This breaks caching, but it&apos;s usually ok because it addresses a very specific use case:</span><br><span class="line">  // using union to union many files or partitions.</span><br><span class="line">  CombineUnions(Union(logicalPlan, other.logicalPlan))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Returns a new Dataset containing rows only in both this Dataset and another Dataset.</span><br><span class="line"> * This is equivalent to `INTERSECT` in SQL.</span><br><span class="line"> *</span><br><span class="line"> * @note Equality checking is performed directly on the encoded representation of the data</span><br><span class="line"> * and thus is not affected by a custom `equals` function defined on `T`.</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">def intersect(other: Dataset[T]): Dataset[T] = withSetOperator &#123;</span><br><span class="line">  Intersect(logicalPlan, other.logicalPlan)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Returns a new Dataset containing rows in this Dataset but not in another Dataset.</span><br><span class="line"> * This is equivalent to `EXCEPT` in SQL.</span><br><span class="line"> *</span><br><span class="line"> * @note Equality checking is performed directly on the encoded representation of the data</span><br><span class="line"> * and thus is not affected by a custom `equals` function defined on `T`.</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">取补集</span><br><span class="line">def except(other: Dataset[T]): Dataset[T] = withSetOperator &#123;</span><br><span class="line">  Except(logicalPlan, other.logicalPlan)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="取样与切分"><a href="#取样与切分" class="headerlink" title="取样与切分"></a>取样与切分</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br></pre></td><td class="code"><pre><span class="line">  /**</span><br><span class="line">   * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed.</span><br><span class="line">   *</span><br><span class="line">   * @param withReplacement Sample with replacement or not.</span><br><span class="line">   * @param fraction Fraction of rows to generate.</span><br><span class="line">   * @param seed Seed for sampling.</span><br><span class="line">   *</span><br><span class="line">   * @note This is NOT guaranteed to provide exactly the fraction of the count</span><br><span class="line">   * of the given [[Dataset]].</span><br><span class="line">   *</span><br><span class="line">   * @group typedrel</span><br><span class="line">   * @since 1.6.0</span><br><span class="line">   */</span><br><span class="line">  def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = &#123;</span><br><span class="line">    require(fraction &gt;= 0,</span><br><span class="line">      s&quot;Fraction must be nonnegative, but got $&#123;fraction&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    withTypedPlan &#123;</span><br><span class="line">      Sample(0.0, fraction, withReplacement, seed, logicalPlan)()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed.</span><br><span class="line">   *</span><br><span class="line">   * @param withReplacement Sample with replacement or not.</span><br><span class="line">   * @param fraction Fraction of rows to generate.</span><br><span class="line">   *</span><br><span class="line">   * @note This is NOT guaranteed to provide exactly the fraction of the total count</span><br><span class="line">   * of the given [[Dataset]].</span><br><span class="line">   *</span><br><span class="line">   * @group typedrel</span><br><span class="line">   * @since 1.6.0</span><br><span class="line">   */</span><br><span class="line">  随即取样</span><br><span class="line">  def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = &#123;</span><br><span class="line">    sample(withReplacement, fraction, Utils.random.nextLong)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Randomly splits this Dataset with the provided weights.</span><br><span class="line">   *</span><br><span class="line">   * @param weights weights for splits, will be normalized if they don&apos;t sum to 1.</span><br><span class="line">   * @param seed Seed for sampling.</span><br><span class="line">   *</span><br><span class="line">   * For Java API, use [[randomSplitAsList]].</span><br><span class="line">   *</span><br><span class="line">   * @group typedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  随即切分???</span><br><span class="line">  def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]] = &#123;</span><br><span class="line">    require(weights.forall(_ &gt;= 0),</span><br><span class="line">      s&quot;Weights must be nonnegative, but got $&#123;weights.mkString(&quot;[&quot;, &quot;,&quot;, &quot;]&quot;)&#125;&quot;)</span><br><span class="line">    require(weights.sum &gt; 0,</span><br><span class="line">      s&quot;Sum of weights must be positive, but got $&#123;weights.mkString(&quot;[&quot;, &quot;,&quot;, &quot;]&quot;)&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    // It is possible that the underlying dataframe doesn&apos;t guarantee the ordering of rows in its</span><br><span class="line">    // constituent partitions each time a split is materialized which could result in</span><br><span class="line">    // overlapping splits. To prevent this, we explicitly sort each input partition to make the</span><br><span class="line">    // ordering deterministic.</span><br><span class="line">    // MapType cannot be sorted.</span><br><span class="line">    val sorted = Sort(logicalPlan.output.filterNot(_.dataType.isInstanceOf[MapType])</span><br><span class="line">      .map(SortOrder(_, Ascending)), global = false, logicalPlan)</span><br><span class="line">    val sum = weights.sum</span><br><span class="line">    val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _)</span><br><span class="line">    normalizedCumWeights.sliding(2).map &#123; x =&gt;</span><br><span class="line">      new Dataset[T](</span><br><span class="line">        sparkSession, Sample(x(0), x(1), withReplacement = false, seed, sorted)(), encoder)</span><br><span class="line">    &#125;.toArray</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Returns a Java list that contains randomly split Dataset with the provided weights.</span><br><span class="line">   *</span><br><span class="line">   * @param weights weights for splits, will be normalized if they don&apos;t sum to 1.</span><br><span class="line">   * @param seed Seed for sampling.</span><br><span class="line">   *</span><br><span class="line">   * @group typedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  def randomSplitAsList(weights: Array[Double], seed: Long): java.util.List[Dataset[T]] = &#123;</span><br><span class="line">    val values = randomSplit(weights, seed)</span><br><span class="line">    java.util.Arrays.asList(values : _*)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Randomly splits this Dataset with the provided weights.</span><br><span class="line">   *</span><br><span class="line">   * @param weights weights for splits, will be normalized if they don&apos;t sum to 1.</span><br><span class="line">   * @group typedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  def randomSplit(weights: Array[Double]): Array[Dataset[T]] = &#123;</span><br><span class="line">    randomSplit(weights, Utils.random.nextLong)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Randomly splits this Dataset with the provided weights. Provided for the Python Api.</span><br><span class="line">   *</span><br><span class="line">   * @param weights weights for splits, will be normalized if they don&apos;t sum to 1.</span><br><span class="line">   * @param seed Seed for sampling.</span><br><span class="line">   */</span><br><span class="line">  private[spark] def randomSplit(weights: List[Double], seed: Long): Array[Dataset[T]] = &#123;</span><br><span class="line">    randomSplit(weights.toArray, seed)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more</span><br><span class="line">   * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of</span><br><span class="line">   * the input row are implicitly joined with each row that is output by the function.</span><br><span class="line">   *</span><br><span class="line">   * Given that this is deprecated, as an alternative, you can explode columns either using</span><br><span class="line">   * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count</span><br><span class="line">   * the number of books that contain a given word:</span><br><span class="line">   *</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   case class Book(title: String, words: String)</span><br><span class="line">   *   val ds: Dataset[Book]</span><br><span class="line">   *</span><br><span class="line">   *   val allWords = ds.select(&apos;title, explode(split(&apos;words, &quot; &quot;)).as(&quot;word&quot;))</span><br><span class="line">   *</span><br><span class="line">   *   val bookCountPerWord = allWords.groupBy(&quot;word&quot;).agg(countDistinct(&quot;title&quot;))</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * Using `flatMap()` this can similarly be exploded as:</span><br><span class="line">   *</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   ds.flatMap(_.words.split(&quot; &quot;))</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * @group untypedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  将字段再处理</span><br><span class="line">  @deprecated(&quot;use flatMap() or select() with functions.explode() instead&quot;, &quot;2.0.0&quot;)</span><br><span class="line">  def explode[A &lt;: Product : TypeTag](input: Column*)(f: Row =&gt; TraversableOnce[A]): DataFrame = &#123;</span><br><span class="line">    val elementSchema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]</span><br><span class="line"></span><br><span class="line">    val convert = CatalystTypeConverters.createToCatalystConverter(elementSchema)</span><br><span class="line"></span><br><span class="line">    val rowFunction =</span><br><span class="line">      f.andThen(_.map(convert(_).asInstanceOf[InternalRow]))</span><br><span class="line">    val generator = UserDefinedGenerator(elementSchema, rowFunction, input.map(_.expr))</span><br><span class="line"></span><br><span class="line">    withPlan &#123;</span><br><span class="line">      Generate(generator, join = true, outer = false,</span><br><span class="line">        qualifier = None, generatorOutput = Nil, logicalPlan)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero</span><br><span class="line">   * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All</span><br><span class="line">   * columns of the input row are implicitly joined with each value that is output by the function.</span><br><span class="line">   *</span><br><span class="line">   * Given that this is deprecated, as an alternative, you can explode columns either using</span><br><span class="line">   * `functions.explode()`:</span><br><span class="line">   *</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   ds.select(explode(split(&apos;words, &quot; &quot;)).as(&quot;word&quot;))</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * or `flatMap()`:</span><br><span class="line">   *</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   ds.flatMap(_.words.split(&quot; &quot;))</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * @group untypedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  @deprecated(&quot;use flatMap() or select() with functions.explode() instead&quot;, &quot;2.0.0&quot;)</span><br><span class="line">  def explode[A, B : TypeTag](inputColumn: String, outputColumn: String)(f: A =&gt; TraversableOnce[B])</span><br><span class="line">    : DataFrame = &#123;</span><br><span class="line">    val dataType = ScalaReflection.schemaFor[B].dataType</span><br><span class="line">    val attributes = AttributeReference(outputColumn, dataType)() :: Nil</span><br><span class="line">    // TODO handle the metadata?</span><br><span class="line">    val elementSchema = attributes.toStructType</span><br><span class="line"></span><br><span class="line">    def rowFunction(row: Row): TraversableOnce[InternalRow] = &#123;</span><br><span class="line">      val convert = CatalystTypeConverters.createToCatalystConverter(dataType)</span><br><span class="line">      f(row(0).asInstanceOf[A]).map(o =&gt; InternalRow(convert(o)))</span><br><span class="line">    &#125;</span><br><span class="line">    val generator = UserDefinedGenerator(elementSchema, rowFunction, apply(inputColumn).expr :: Nil)</span><br><span class="line"></span><br><span class="line">    withPlan &#123;</span><br><span class="line">      Generate(generator, join = true, outer = false,</span><br><span class="line">        qualifier = None, generatorOutput = Nil, logicalPlan)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">## 列操作</span><br><span class="line">  /**</span><br><span class="line">   * Returns a new Dataset by adding a column or replacing the existing column that has</span><br><span class="line">   * the same name.</span><br><span class="line">   *</span><br><span class="line">   * @group untypedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  列操作</span><br><span class="line">  def withColumn(colName: String, col: Column): DataFrame = &#123;</span><br><span class="line">    val resolver = sparkSession.sessionState.analyzer.resolver</span><br><span class="line">    val output = queryExecution.analyzed.output</span><br><span class="line">    val shouldReplace = output.exists(f =&gt; resolver(f.name, colName))</span><br><span class="line">    if (shouldReplace) &#123;</span><br><span class="line">      val columns = output.map &#123; field =&gt;</span><br><span class="line">        if (resolver(field.name, colName)) &#123;</span><br><span class="line">          col.as(colName)</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          Column(field)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      select(columns : _*)</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      select(Column(&quot;*&quot;), col.as(colName))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Returns a new Dataset by adding a column with metadata.</span><br><span class="line">   */</span><br><span class="line">  private[spark] def withColumn(colName: String, col: Column, metadata: Metadata): DataFrame = &#123;</span><br><span class="line">    withColumn(colName, col.as(colName, metadata))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Returns a new Dataset with a column renamed.</span><br><span class="line">   * This is a no-op if schema doesn&apos;t contain existingName.</span><br><span class="line">   *</span><br><span class="line">   * @group untypedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  def withColumnRenamed(existingName: String, newName: String): DataFrame = &#123;</span><br><span class="line">    val resolver = sparkSession.sessionState.analyzer.resolver</span><br><span class="line">    val output = queryExecution.analyzed.output</span><br><span class="line">    val shouldRename = output.exists(f =&gt; resolver(f.name, existingName))</span><br><span class="line">    if (shouldRename) &#123;</span><br><span class="line">      val columns = output.map &#123; col =&gt;</span><br><span class="line">        if (resolver(col.name, existingName)) &#123;</span><br><span class="line">          Column(col).as(newName)</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          Column(col)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      select(columns : _*)</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      toDF()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Returns a new Dataset with a column dropped. This is a no-op if schema doesn&apos;t contain</span><br><span class="line">   * column name.</span><br><span class="line">   *</span><br><span class="line">   * This method can only be used to drop top level columns. the colName string is treated</span><br><span class="line">   * literally without further interpretation.</span><br><span class="line">   *</span><br><span class="line">   * @group untypedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  删除指定列</span><br><span class="line">  def drop(colName: String): DataFrame = &#123;</span><br><span class="line">    drop(Seq(colName) : _*)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Returns a new Dataset with columns dropped.</span><br><span class="line">   * This is a no-op if schema doesn&apos;t contain column name(s).</span><br><span class="line">   *</span><br><span class="line">   * This method can only be used to drop top level columns. the colName string is treated literally</span><br><span class="line">   * without further interpretation.</span><br><span class="line">   *</span><br><span class="line">   * @group untypedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  @scala.annotation.varargs</span><br><span class="line">  def drop(colNames: String*): DataFrame = &#123;</span><br><span class="line">    val resolver = sparkSession.sessionState.analyzer.resolver</span><br><span class="line">    val allColumns = queryExecution.analyzed.output</span><br><span class="line">    val remainingCols = allColumns.filter &#123; attribute =&gt;</span><br><span class="line">      colNames.forall(n =&gt; !resolver(attribute.name, n))</span><br><span class="line">    &#125;.map(attribute =&gt; Column(attribute))</span><br><span class="line">    if (remainingCols.size == allColumns.size) &#123;</span><br><span class="line">      toDF()</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      this.select(remainingCols: _*)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Returns a new Dataset with a column dropped.</span><br><span class="line">   * This version of drop accepts a [[Column]] rather than a name.</span><br><span class="line">   * This is a no-op if the Dataset doesn&apos;t have a column</span><br><span class="line">   * with an equivalent expression.</span><br><span class="line">   *</span><br><span class="line">   * @group untypedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  def drop(col: Column): DataFrame = &#123;</span><br><span class="line">    val expression = col match &#123;</span><br><span class="line">      case Column(u: UnresolvedAttribute) =&gt;</span><br><span class="line">        queryExecution.analyzed.resolveQuoted(</span><br><span class="line">          u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u)</span><br><span class="line">      case Column(expr: Expression) =&gt; expr</span><br><span class="line">    &#125;</span><br><span class="line">    val attrs = this.logicalPlan.output</span><br><span class="line">    val colsAfterDrop = attrs.filter &#123; attr =&gt;</span><br><span class="line">      attr != expression</span><br><span class="line">    &#125;.map(attr =&gt; Column(attr))</span><br><span class="line">    select(colsAfterDrop : _*)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h2 id="去重"><a href="#去重" class="headerlink" title="去重"></a>去重</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Returns a new Dataset that contains only the unique rows from this Dataset.</span><br><span class="line"> * This is an alias for `distinct`.</span><br><span class="line"> *</span><br><span class="line"> * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it</span><br><span class="line"> * will keep all data across triggers as intermediate state to drop duplicates rows. You can use</span><br><span class="line"> * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit</span><br><span class="line"> * the state. In addition, too late data older than watermark will be dropped to avoid any</span><br><span class="line"> * possibility of duplicates.</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">删除重复的行</span><br><span class="line">def dropDuplicates(): Dataset[T] = dropDuplicates(this.columns)</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only</span><br><span class="line"> * the subset of columns.</span><br><span class="line"> *</span><br><span class="line"> * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it</span><br><span class="line"> * will keep all data across triggers as intermediate state to drop duplicates rows. You can use</span><br><span class="line"> * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit</span><br><span class="line"> * the state. In addition, too late data older than watermark will be dropped to avoid any</span><br><span class="line"> * possibility of duplicates.</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">def dropDuplicates(colNames: Seq[String]): Dataset[T] = withTypedPlan &#123;</span><br><span class="line">  val resolver = sparkSession.sessionState.analyzer.resolver</span><br><span class="line">  val allColumns = queryExecution.analyzed.output</span><br><span class="line">  val groupCols = colNames.toSet.toSeq.flatMap &#123; (colName: String) =&gt;</span><br><span class="line">    // It is possibly there are more than one columns with the same name,</span><br><span class="line">    // so we call filter instead of find.</span><br><span class="line">    val cols = allColumns.filter(col =&gt; resolver(col.name, colName))</span><br><span class="line">    if (cols.isEmpty) &#123;</span><br><span class="line">      throw new AnalysisException(</span><br><span class="line">        s&quot;&quot;&quot;Cannot resolve column name &quot;$colName&quot; among ($&#123;schema.fieldNames.mkString(&quot;, &quot;)&#125;)&quot;&quot;&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">    cols</span><br><span class="line">  &#125;</span><br><span class="line">  Deduplicate(groupCols, logicalPlan, isStreaming)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Returns a new Dataset with duplicate rows removed, considering only</span><br><span class="line"> * the subset of columns.</span><br><span class="line"> *</span><br><span class="line"> * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it</span><br><span class="line"> * will keep all data across triggers as intermediate state to drop duplicates rows. You can use</span><br><span class="line"> * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit</span><br><span class="line"> * the state. In addition, too late data older than watermark will be dropped to avoid any</span><br><span class="line"> * possibility of duplicates.</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">def dropDuplicates(colNames: Array[String]): Dataset[T] = dropDuplicates(colNames.toSeq)</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Returns a new [[Dataset]] with duplicate rows removed, considering only</span><br><span class="line"> * the subset of columns.</span><br><span class="line"> *</span><br><span class="line"> * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it</span><br><span class="line"> * will keep all data across triggers as intermediate state to drop duplicates rows. You can use</span><br><span class="line"> * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit</span><br><span class="line"> * the state. In addition, too late data older than watermark will be dropped to avoid any</span><br><span class="line"> * possibility of duplicates.</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">@scala.annotation.varargs</span><br><span class="line">def dropDuplicates(col1: String, cols: String*): Dataset[T] = &#123;</span><br><span class="line">  val colNames: Seq[String] = col1 +: cols</span><br><span class="line">  dropDuplicates(colNames)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="统计指定的列"><a href="#统计指定的列" class="headerlink" title="统计指定的列"></a>统计指定的列</h2><pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Computes statistics for numeric and string columns, including count, mean, stddev, min, and</span><br><span class="line"> * max. If no columns are given, this function computes statistics for all numerical or string</span><br><span class="line"> * columns.</span><br><span class="line"> *</span><br><span class="line"> * This function is meant for exploratory data analysis, as we make no guarantee about the</span><br><span class="line"> * backward compatibility of the schema of the resulting Dataset. If you want to</span><br><span class="line"> * programmatically compute summary statistics, use the `agg` function instead.</span><br><span class="line"> *</span><br><span class="line"> * &#123;&#123;&#123;</span><br><span class="line"> *   ds.describe(&quot;age&quot;, &quot;height&quot;).show()</span><br><span class="line"> *</span><br><span class="line"> *   // output:</span><br><span class="line"> *   // summary age   height</span><br><span class="line"> *   // count   10.0  10.0</span><br><span class="line"> *   // mean    53.3  178.05</span><br><span class="line"> *   // stddev  11.6  15.7</span><br><span class="line"> *   // min     18.0  163.0</span><br><span class="line"> *   // max     92.0  192.0</span><br><span class="line"> * &#125;&#125;&#125;</span><br><span class="line"> *</span><br><span class="line"> * @group action</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">获得指定列的描述性统计量</span><br><span class="line">@scala.annotation.varargs</span><br><span class="line">def describe(cols: String*): DataFrame = withPlan &#123;</span><br><span class="line"></span><br><span class="line">  // The list of summary statistics to compute, in the form of expressions.</span><br><span class="line">  val statistics = List[(String, Expression =&gt; Expression)](</span><br><span class="line">    &quot;count&quot; -&gt; ((child: Expression) =&gt; Count(child).toAggregateExpression()),</span><br><span class="line">    &quot;mean&quot; -&gt; ((child: Expression) =&gt; Average(child).toAggregateExpression()),</span><br><span class="line">    &quot;stddev&quot; -&gt; ((child: Expression) =&gt; StddevSamp(child).toAggregateExpression()),</span><br><span class="line">    &quot;min&quot; -&gt; ((child: Expression) =&gt; Min(child).toAggregateExpression()),</span><br><span class="line">    &quot;max&quot; -&gt; ((child: Expression) =&gt; Max(child).toAggregateExpression()))</span><br><span class="line"></span><br><span class="line">  val outputCols =</span><br><span class="line">    (if (cols.isEmpty) aggregatableColumns.map(usePrettyExpression(_).sql) else cols).toList</span><br><span class="line"></span><br><span class="line">  val ret: Seq[Row] = if (outputCols.nonEmpty) &#123;</span><br><span class="line">    val aggExprs = statistics.flatMap &#123; case (_, colToAgg) =&gt;</span><br><span class="line">      outputCols.map(c =&gt; Column(Cast(colToAgg(Column(c).expr), StringType)).as(c))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    val row = groupBy().agg(aggExprs.head, aggExprs.tail: _*).head().toSeq</span><br><span class="line"></span><br><span class="line">    // Pivot the data so each summary is one row</span><br><span class="line">    row.grouped(outputCols.size).toSeq.zip(statistics).map &#123; case (aggregation, (statistic, _)) =&gt;</span><br><span class="line">      Row(statistic :: aggregation.toList: _*)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    // If there are no output columns, just output a single column that contains the stats.</span><br><span class="line">    statistics.map &#123; case (name, _) =&gt; Row(name) &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  // All columns are string type</span><br><span class="line">  val schema = StructType(</span><br><span class="line">    StructField(&quot;summary&quot;, StringType) :: outputCols.map(StructField(_, StringType))).toAttributes</span><br><span class="line">  // `toArray` forces materialization to make the seq serializable</span><br><span class="line">  LocalRelation.fromExternalRows(schema, ret.toArray.toSeq)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Returns the first `n` rows.</span><br><span class="line"> *</span><br><span class="line"> * @note this method should only be used if the resulting array is expected to be small, as</span><br><span class="line"> * all the data is loaded into the driver&apos;s memory.</span><br><span class="line"> *</span><br><span class="line"> * @group action</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">取得前n行数据</span><br><span class="line">def head(n: Int): Array[T] = withAction(&quot;head&quot;, limit(n).queryExecution)(collectFromPlan)</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Returns the first row.</span><br><span class="line"> * @group action</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">def head(): T = head(1).head</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Returns the first row. Alias for head().</span><br><span class="line"> * @group action</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">def first(): T = head()</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Concise syntax for chaining custom transformations.</span><br><span class="line"> * &#123;&#123;&#123;</span><br><span class="line"> *   def featurize(ds: Dataset[T]): Dataset[U] = ...</span><br><span class="line"> *</span><br><span class="line"> *   ds</span><br><span class="line"> *     .transform(featurize)</span><br><span class="line"> *     .transform(...)</span><br><span class="line"> * &#125;&#125;&#125;</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">转换？？</span><br><span class="line">def transform[U](t: Dataset[T] =&gt; Dataset[U]): Dataset[U] = t(this)</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * :: Experimental ::</span><br><span class="line"> * (Scala-specific)</span><br><span class="line"> * Returns a new Dataset that only contains elements where `func` returns `true`.</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">过滤</span><br><span class="line">@Experimental</span><br><span class="line">@InterfaceStability.Evolving</span><br><span class="line">def filter(func: T =&gt; Boolean): Dataset[T] = &#123;</span><br><span class="line">  withTypedPlan(TypedFilter(func, logicalPlan))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * :: Experimental ::</span><br><span class="line"> * (Java-specific)</span><br><span class="line"> * Returns a new Dataset that only contains elements where `func` returns `true`.</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">@Experimental</span><br><span class="line">@InterfaceStability.Evolving</span><br><span class="line">def filter(func: FilterFunction[T]): Dataset[T] = &#123;</span><br><span class="line">  withTypedPlan(TypedFilter(func, logicalPlan))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</code></pre><h2 id="数据的转换"><a href="#数据的转换" class="headerlink" title="数据的转换"></a>数据的转换</h2>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * :: Experimental ::</span><br><span class="line"> * (Scala-specific)</span><br><span class="line"> * Returns a new Dataset that contains the result of applying `func` to each element.</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">映射操作</span><br><span class="line">@Experimental</span><br><span class="line">@InterfaceStability.Evolving</span><br><span class="line">def map[U : Encoder](func: T =&gt; U): Dataset[U] = withTypedPlan &#123;</span><br><span class="line">  MapElements[T, U](func, logicalPlan)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * :: Experimental ::</span><br><span class="line"> * (Java-specific)</span><br><span class="line"> * Returns a new Dataset that contains the result of applying `func` to each element.</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">@Experimental</span><br><span class="line">@InterfaceStability.Evolving</span><br><span class="line">def map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U] = &#123;</span><br><span class="line">  implicit val uEnc = encoder</span><br><span class="line">  withTypedPlan(MapElements[T, U](func, logicalPlan))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * :: Experimental ::</span><br><span class="line"> * (Scala-specific)</span><br><span class="line"> * Returns a new Dataset that contains the result of applying `func` to each partition.</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">@Experimental</span><br><span class="line">@InterfaceStability.Evolving</span><br><span class="line">def mapPartitions[U : Encoder](func: Iterator[T] =&gt; Iterator[U]): Dataset[U] = &#123;</span><br><span class="line">  new Dataset[U](</span><br><span class="line">    sparkSession,</span><br><span class="line">    MapPartitions[T, U](func, logicalPlan),</span><br><span class="line">    implicitly[Encoder[U]])</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * :: Experimental ::</span><br><span class="line"> * (Java-specific)</span><br><span class="line"> * Returns a new Dataset that contains the result of applying `f` to each partition.</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">@Experimental</span><br><span class="line">@InterfaceStability.Evolving</span><br><span class="line">def mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U] = &#123;</span><br><span class="line">  val func: (Iterator[T]) =&gt; Iterator[U] = x =&gt; f.call(x.asJava).asScala</span><br><span class="line">  mapPartitions(func)(encoder)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Returns a new `DataFrame` that contains the result of applying a serialized R function</span><br><span class="line"> * `func` to each partition.</span><br><span class="line"> */</span><br><span class="line">private[sql] def mapPartitionsInR(</span><br><span class="line">    func: Array[Byte],</span><br><span class="line">    packageNames: Array[Byte],</span><br><span class="line">    broadcastVars: Array[Broadcast[Object]],</span><br><span class="line">    schema: StructType): DataFrame = &#123;</span><br><span class="line">  val rowEncoder = encoder.asInstanceOf[ExpressionEncoder[Row]]</span><br><span class="line">  Dataset.ofRows(</span><br><span class="line">    sparkSession,</span><br><span class="line">    MapPartitionsInR(func, packageNames, broadcastVars, schema, rowEncoder, logicalPlan))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * :: Experimental ::</span><br><span class="line"> * (Scala-specific)</span><br><span class="line"> * Returns a new Dataset by first applying a function to all elements of this Dataset,</span><br><span class="line"> * and then flattening the results.</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">@Experimental</span><br><span class="line">@InterfaceStability.Evolving</span><br><span class="line">def flatMap[U : Encoder](func: T =&gt; TraversableOnce[U]): Dataset[U] =</span><br><span class="line">  mapPartitions(_.flatMap(func))</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * :: Experimental ::</span><br><span class="line"> * (Java-specific)</span><br><span class="line"> * Returns a new Dataset by first applying a function to all elements of this Dataset,</span><br><span class="line"> * and then flattening the results.</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">@Experimental</span><br><span class="line">@InterfaceStability.Evolving</span><br><span class="line">def flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U] = &#123;</span><br><span class="line">  val func: (T) =&gt; Iterator[U] = x =&gt; f.call(x).asScala</span><br><span class="line">  flatMap(func)(encoder)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Applies a function `f` to all rows.</span><br><span class="line"> *</span><br><span class="line"> * @group action</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">def foreach(f: T =&gt; Unit): Unit = withNewExecutionId &#123;</span><br><span class="line">  rdd.foreach(f)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * (Java-specific)</span><br><span class="line"> * Runs `func` on each element of this Dataset.</span><br><span class="line"> *</span><br><span class="line"> * @group action</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">def foreach(func: ForeachFunction[T]): Unit = foreach(func.call(_))</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Applies a function `f` to each partition of this Dataset.</span><br><span class="line"> *</span><br><span class="line"> * @group action</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">def foreachPartition(f: Iterator[T] =&gt; Unit): Unit = withNewExecutionId &#123;</span><br><span class="line">  rdd.foreachPartition(f)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="数据的提取与聚合"><a href="#数据的提取与聚合" class="headerlink" title="数据的提取与聚合"></a>数据的提取与聚合</h2>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line">  /**</span><br><span class="line">   * (Java-specific)</span><br><span class="line">   * Runs `func` on each partition of this Dataset.</span><br><span class="line">   *</span><br><span class="line">   * @group action</span><br><span class="line">   * @since 1.6.0</span><br><span class="line">   */</span><br><span class="line">  def foreachPartition(func: ForeachPartitionFunction[T]): Unit =</span><br><span class="line">    foreachPartition(it =&gt; func.call(it.asJava))</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Returns the first `n` rows in the Dataset.</span><br><span class="line">   *</span><br><span class="line">   * Running take requires moving data into the application&apos;s driver process, and doing so with</span><br><span class="line">   * a very large `n` can crash the driver process with OutOfMemoryError.</span><br><span class="line">   *</span><br><span class="line">   * @group action</span><br><span class="line">   * @since 1.6.0</span><br><span class="line">   */</span><br><span class="line">  def take(n: Int): Array[T] = head(n)</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Returns the first `n` rows in the Dataset as a list.</span><br><span class="line">   *</span><br><span class="line">   * Running take requires moving data into the application&apos;s driver process, and doing so with</span><br><span class="line">   * a very large `n` can crash the driver process with OutOfMemoryError.</span><br><span class="line">   *</span><br><span class="line">   * @group action</span><br><span class="line">   * @since 1.6.0</span><br><span class="line">   */</span><br><span class="line">    获取数据成一个列表</span><br><span class="line">  def takeAsList(n: Int): java.util.List[T] = java.util.Arrays.asList(take(n) : _*)</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Returns an array that contains all rows in this Dataset.</span><br><span class="line">   *</span><br><span class="line">   * Running collect requires moving all the data into the application&apos;s driver process, and</span><br><span class="line">   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.</span><br><span class="line">   *</span><br><span class="line">   * For Java API, use [[collectAsList]].</span><br><span class="line">   *</span><br><span class="line">   * @group action</span><br><span class="line">   * @since 1.6.0</span><br><span class="line">   */</span><br><span class="line"></span><br><span class="line">统计数据</span><br><span class="line">  def collect(): Array[T] = withAction(&quot;collect&quot;, queryExecution)(collectFromPlan)</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Returns a Java list that contains all rows in this Dataset.</span><br><span class="line">   *</span><br><span class="line">   * Running collect requires moving all the data into the application&apos;s driver process, and</span><br><span class="line">   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.</span><br><span class="line">   *</span><br><span class="line">   * @group action</span><br><span class="line">   * @since 1.6.0</span><br><span class="line">   */</span><br><span class="line">  def collectAsList(): java.util.List[T] = withAction(&quot;collectAsList&quot;, queryExecution) &#123; plan =&gt;</span><br><span class="line">    val values = collectFromPlan(plan)</span><br><span class="line">    java.util.Arrays.asList(values : _*)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Return an iterator that contains all rows in this Dataset.</span><br><span class="line">   *</span><br><span class="line">   * The iterator will consume as much memory as the largest partition in this Dataset.</span><br><span class="line">   *</span><br><span class="line">   * @note this results in multiple Spark jobs, and if the input Dataset is the result</span><br><span class="line">   * of a wide transformation (e.g. join with different partitioners), to avoid</span><br><span class="line">   * recomputing the input Dataset should be cached first.</span><br><span class="line">   *</span><br><span class="line">   * @group action</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  def toLocalIterator(): java.util.Iterator[T] = &#123;</span><br><span class="line">    withAction(&quot;toLocalIterator&quot;, queryExecution) &#123; plan =&gt;</span><br><span class="line">      plan.executeToIterator().map(boundEnc.fromRow).asJava</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Returns the number of rows in the Dataset.</span><br><span class="line">   * @group action</span><br><span class="line">   * @since 1.6.0</span><br><span class="line">   */</span><br><span class="line">  统计行</span><br><span class="line">  def count(): Long = withAction(&quot;count&quot;, groupBy().count().queryExecution) &#123; plan =&gt;</span><br><span class="line">    plan.executeCollect().head.getLong(0)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h2 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h2><pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Returns a new Dataset that has exactly `numPartitions` partitions.</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">分区</span><br><span class="line">def repartition(numPartitions: Int): Dataset[T] = withTypedPlan &#123;</span><br><span class="line">  Repartition(numPartitions, shuffle = true, logicalPlan)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Returns a new Dataset partitioned by the given partitioning expressions into</span><br><span class="line"> * `numPartitions`. The resulting Dataset is hash partitioned.</span><br><span class="line"> *</span><br><span class="line"> * This is the same operation as &quot;DISTRIBUTE BY&quot; in SQL (Hive QL).</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">@scala.annotation.varargs</span><br><span class="line">def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T] = withTypedPlan &#123;</span><br><span class="line">  RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, numPartitions)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Returns a new Dataset partitioned by the given partitioning expressions, using</span><br><span class="line"> * `spark.sql.shuffle.partitions` as number of partitions.</span><br><span class="line"> * The resulting Dataset is hash partitioned.</span><br><span class="line"> *</span><br><span class="line"> * This is the same operation as &quot;DISTRIBUTE BY&quot; in SQL (Hive QL).</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">@scala.annotation.varargs</span><br><span class="line">def repartition(partitionExprs: Column*): Dataset[T] = withTypedPlan &#123;</span><br><span class="line">  RepartitionByExpression(</span><br><span class="line">    partitionExprs.map(_.expr), logicalPlan, sparkSession.sessionState.conf.numShufflePartitions)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Returns a new Dataset that has exactly `numPartitions` partitions.</span><br><span class="line"> * Similar to coalesce defined on an `RDD`, this operation results in a narrow dependency, e.g.</span><br><span class="line"> * if you go from 1000 partitions to 100 partitions, there will not be a shuffle, instead each of</span><br><span class="line"> * the 100 new partitions will claim 10 of the current partitions.  If a larger number of</span><br><span class="line"> * partitions is requested, it will stay at the current number of partitions.</span><br><span class="line"> *</span><br><span class="line"> * However, if you&apos;re doing a drastic coalesce, e.g. to numPartitions = 1,</span><br><span class="line"> * this may result in your computation taking place on fewer nodes than</span><br><span class="line"> * you like (e.g. one node in the case of numPartitions = 1). To avoid this,</span><br><span class="line"> * you can call repartition. This will add a shuffle step, but means the</span><br><span class="line"> * current upstream partitions will be executed in parallel (per whatever</span><br><span class="line"> * the current partitioning is).</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">def coalesce(numPartitions: Int): Dataset[T] = withTypedPlan &#123;</span><br><span class="line">  Repartition(numPartitions, shuffle = false, logicalPlan)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Returns a new Dataset that contains only the unique rows from this Dataset.</span><br><span class="line"> * This is an alias for `dropDuplicates`.</span><br><span class="line"> *</span><br><span class="line"> * @note Equality checking is performed directly on the encoded representation of the data</span><br><span class="line"> * and thus is not affected by a custom `equals` function defined on `T`.</span><br><span class="line"> *</span><br><span class="line"> * @group typedrel</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">def distinct(): Dataset[T] = dropDuplicates()</span><br></pre></td></tr></table></figure>
</code></pre><h2 id="数据的持久化"><a href="#数据的持久化" class="headerlink" title="数据的持久化"></a>数据的持久化</h2><p>  数据的持久化和缓存策略，一般我们操作rdd都是延迟计算，但是当我们多次重复使用一个rdd的时候可以选择将其缓存而不是每次进行一个计算，可以提高效率。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).</span><br><span class="line"> *</span><br><span class="line"> * @group basic</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">                持久化</span><br><span class="line">def persist(): this.type = &#123;</span><br><span class="line">  sparkSession.sharedState.cacheManager.cacheQuery(this)</span><br><span class="line">  this</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).</span><br><span class="line"> *</span><br><span class="line"> * @group basic</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">def cache(): this.type = persist()</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Persist this Dataset with the given storage level.</span><br><span class="line"> * @param newLevel One of: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`,</span><br><span class="line"> *                 `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`,</span><br><span class="line"> *                 `MEMORY_AND_DISK_2`, etc.</span><br><span class="line"> *</span><br><span class="line"> * @group basic</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">def persist(newLevel: StorageLevel): this.type = &#123;</span><br><span class="line">  sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel)</span><br><span class="line">  this</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Get the Dataset&apos;s current storage level, or StorageLevel.NONE if not persisted.</span><br><span class="line"> *</span><br><span class="line"> * @group basic</span><br><span class="line"> * @since 2.1.0</span><br><span class="line"> */</span><br><span class="line">def storageLevel: StorageLevel = &#123;</span><br><span class="line">  sparkSession.sharedState.cacheManager.lookupCachedData(this).map &#123; cachedData =&gt;</span><br><span class="line">    cachedData.cachedRepresentation.storageLevel</span><br><span class="line">  &#125;.getOrElse(StorageLevel.NONE)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.</span><br><span class="line"> *</span><br><span class="line"> * @param blocking Whether to block until all blocks are deleted.</span><br><span class="line"> *</span><br><span class="line"> * @group basic</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">def unpersist(blocking: Boolean): this.type = &#123;</span><br><span class="line">  sparkSession.sharedState.cacheManager.uncacheQuery(this, blocking)</span><br><span class="line">  this</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.</span><br><span class="line"> *</span><br><span class="line"> * @group basic</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">def unpersist(): this.type = unpersist(blocking = false)</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Represents the content of the Dataset as an `RDD` of `T`.</span><br><span class="line"> *</span><br><span class="line"> * @group basic</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">lazy val rdd: RDD[T] = &#123;</span><br><span class="line">  val objectType = exprEnc.deserializer.dataType</span><br><span class="line">  val deserialized = CatalystSerde.deserialize[T](logicalPlan)</span><br><span class="line">  sparkSession.sessionState.executePlan(deserialized).toRdd.mapPartitions &#123; rows =&gt;</span><br><span class="line">    rows.map(_.get(0, objectType).asInstanceOf[T])</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Returns the content of the Dataset as a `JavaRDD` of `T`s.</span><br><span class="line"> * @group basic</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">def toJavaRDD: JavaRDD[T] = rdd.toJavaRDD()</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Returns the content of the Dataset as a `JavaRDD` of `T`s.</span><br><span class="line"> * @group basic</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">def javaRDD: JavaRDD[T] = toJavaRDD</span><br></pre></td></tr></table></figure>
<h2 id="注册临时表"><a href="#注册临时表" class="headerlink" title="注册临时表"></a>注册临时表</h2><p>通过注册可以将一个dataset直接当作一个表来操作，这样就可以直接通过sql来执行了，不过返回的结果又是一个dataset<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Registers this Dataset as a temporary table using the given name. The lifetime of this</span><br><span class="line"> * temporary table is tied to the [[SparkSession]] that was used to create this Dataset.</span><br><span class="line"> *</span><br><span class="line"> * @group basic</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">注册</span><br><span class="line">@deprecated(&quot;Use createOrReplaceTempView(viewName) instead.&quot;, &quot;2.0.0&quot;)</span><br><span class="line">def registerTempTable(tableName: String): Unit = &#123;</span><br><span class="line">  createOrReplaceTempView(tableName)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Creates a local temporary view using the given name. The lifetime of this</span><br><span class="line"> * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.</span><br><span class="line"> *</span><br><span class="line"> * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that</span><br><span class="line"> * created it, i.e. it will be automatically dropped when the session terminates. It&apos;s not</span><br><span class="line"> * tied to any databases, i.e. we can&apos;t use `db1.view1` to reference a local temporary view.</span><br><span class="line"> *</span><br><span class="line"> * @throws AnalysisException if the view name is invalid or already exists</span><br><span class="line"> *</span><br><span class="line"> * @group basic</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">创建表</span><br><span class="line">@throws[AnalysisException]</span><br><span class="line">def createTempView(viewName: String): Unit = withPlan &#123;</span><br><span class="line">  createTempViewCommand(viewName, replace = false, global = false)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Creates a local temporary view using the given name. The lifetime of this</span><br><span class="line"> * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.</span><br><span class="line"> *</span><br><span class="line"> * @group basic</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">def createOrReplaceTempView(viewName: String): Unit = withPlan &#123;</span><br><span class="line">  createTempViewCommand(viewName, replace = true, global = false)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Creates a global temporary view using the given name. The lifetime of this</span><br><span class="line"> * temporary view is tied to this Spark application.</span><br><span class="line"> *</span><br><span class="line"> * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,</span><br><span class="line"> * i.e. it will be automatically dropped when the application terminates. It&apos;s tied to a system</span><br><span class="line"> * preserved database `global_temp`, and we must use the qualified name to refer a global temp</span><br><span class="line"> * view, e.g. `SELECT * FROM global_temp.view1`.</span><br><span class="line"> *</span><br><span class="line"> * @throws AnalysisException if the view name is invalid or already exists</span><br><span class="line"> *</span><br><span class="line"> * @group basic</span><br><span class="line"> * @since 2.1.0</span><br><span class="line"> */</span><br><span class="line">@throws[AnalysisException]</span><br><span class="line">def createGlobalTempView(viewName: String): Unit = withPlan &#123;</span><br><span class="line">  createTempViewCommand(viewName, replace = false, global = true)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private def createTempViewCommand(</span><br><span class="line">    viewName: String,</span><br><span class="line">    replace: Boolean,</span><br><span class="line">    global: Boolean): CreateViewCommand = &#123;</span><br><span class="line">  val viewType = if (global) GlobalTempView else LocalTempView</span><br><span class="line"></span><br><span class="line">  val tableIdentifier = try &#123;</span><br><span class="line">    sparkSession.sessionState.sqlParser.parseTableIdentifier(viewName)</span><br><span class="line">  &#125; catch &#123;</span><br><span class="line">    case _: ParseException =&gt; throw new AnalysisException(s&quot;Invalid view name: $viewName&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">  CreateViewCommand(</span><br><span class="line">    name = tableIdentifier,</span><br><span class="line">    userSpecifiedColumns = Nil,</span><br><span class="line">    comment = None,</span><br><span class="line">    properties = Map.empty,</span><br><span class="line">    originalText = None,</span><br><span class="line">    child = logicalPlan,</span><br><span class="line">    allowExisting = false,</span><br><span class="line">    replace = replace,</span><br><span class="line">    viewType = viewType)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="数据保存"><a href="#数据保存" class="headerlink" title="数据保存"></a>数据保存</h2><pre><code>数据保存有一个专门的write类来处理，这里就是调用write方法返回一个write对象来实现的
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Interface for saving the content of the non-streaming Dataset out into external storage.</span><br><span class="line"> *</span><br><span class="line"> * @group basic</span><br><span class="line"> * @since 1.6.0</span><br><span class="line"> */</span><br><span class="line">def write: DataFrameWriter[T] = &#123;</span><br><span class="line">  if (isStreaming) &#123;</span><br><span class="line">    logicalPlan.failAnalysis(</span><br><span class="line">      &quot;&apos;write&apos; can not be called on streaming Dataset/DataFrame&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">  new DataFrameWriter[T](this)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * :: Experimental ::</span><br><span class="line"> * Interface for saving the content of the streaming Dataset out into external storage.</span><br><span class="line"> *</span><br><span class="line"> * @group basic</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">@Experimental</span><br><span class="line">@InterfaceStability.Evolving</span><br><span class="line">def writeStream: DataStreamWriter[T] = &#123;</span><br><span class="line">  if (!isStreaming) &#123;</span><br><span class="line">    logicalPlan.failAnalysis(</span><br><span class="line">      &quot;&apos;writeStream&apos; can be called only on streaming Dataset/DataFrame&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">  new DataStreamWriter[T](this)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="转换成json格式"><a href="#转换成json格式" class="headerlink" title="转换成json格式"></a>转换成json格式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Returns the content of the Dataset as a Dataset of JSON strings.</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">def toJSON: Dataset[String] = &#123;</span><br><span class="line">  val rowSchema = this.schema</span><br><span class="line">  val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone</span><br><span class="line">  val rdd: RDD[String] = queryExecution.toRdd.mapPartitions &#123; iter =&gt;</span><br><span class="line">    val writer = new CharArrayWriter()</span><br><span class="line">    // create the Generator without separator inserted between 2 records</span><br><span class="line">    val gen = new JacksonGenerator(rowSchema, writer,</span><br><span class="line">      new JSONOptions(Map.empty[String, String], sessionLocalTimeZone))</span><br><span class="line"></span><br><span class="line">    new Iterator[String] &#123;</span><br><span class="line">      override def hasNext: Boolean = iter.hasNext</span><br><span class="line">      override def next(): String = &#123;</span><br><span class="line">        gen.write(iter.next())</span><br><span class="line">        gen.flush()</span><br><span class="line"></span><br><span class="line">        val json = writer.toString</span><br><span class="line">        if (hasNext) &#123;</span><br><span class="line">          writer.reset()</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          gen.close()</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        json</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  import sparkSession.implicits.newStringEncoder</span><br><span class="line">  sparkSession.createDataset(rdd)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="获取文件列表"><a href="#获取文件列表" class="headerlink" title="获取文件列表"></a>获取文件列表</h2><p>  可以获取当前dataSet都加载了那些文件<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Returns a best-effort snapshot of the files that compose this Dataset. This method simply</span><br><span class="line"> * asks each constituent BaseRelation for its respective files and takes the union of all results.</span><br><span class="line"> * Depending on the source relations, this may not find all input files. Duplicates are removed.</span><br><span class="line"> *</span><br><span class="line"> * @group basic</span><br><span class="line"> * @since 2.0.0</span><br><span class="line"> */</span><br><span class="line">def inputFiles: Array[String] = &#123;</span><br><span class="line">  val files: Seq[String] = queryExecution.optimizedPlan.collect &#123;</span><br><span class="line">    case LogicalRelation(fsBasedRelation: FileRelation, _, _) =&gt;</span><br><span class="line">      fsBasedRelation.inputFiles</span><br><span class="line">    case fr: FileRelation =&gt;</span><br><span class="line">      fr.inputFiles</span><br><span class="line">  &#125;.flatten</span><br><span class="line">  files.toSet.toArray</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
    <div>坚持原创技术分享，您的支持将鼓励我继续创作！</div>
    <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
      <span>赏</span>
    </button>
    <div id="QR" style="display: none;">
      
        <div id="wechat" style="display: inline-block">
          <img id="wechat_qr" src="/blog/images/mm.jpg" alt="hxfeng WeChat Pay"/>
          <p>微信打赏</p>
        </div>
      
      
        <div id="alipay" style="display: inline-block">
          <img id="alipay_qr" src="/blog/images/zfb.jpg" alt="hxfeng Alipay"/>
          <p>支付宝打赏</p>
        </div>
      
    </div>
  </div>


      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/blog/tags/spark/" rel="tag"># spark</a>
          
            <a href="/blog/tags/大数据/" rel="tag"># 大数据</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/blog/2017/03/10/dadaozhijian/" rel="next" title="大道至简之绘画">
                <i class="fa fa-chevron-left"></i> 大道至简之绘画
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/blog/2017/03/12/groupbyandrollup/" rel="prev" title="group by rollup 和 cube的原理分析">
                group by rollup 和 cube的原理分析 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

          
          </div>
          


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2017/03/11/spark1/"
           data-title="spark 源码分析之 sparksql DataSet" data-url="https://hxfeng.github.io/2017/03/11/spark1/">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/blog/images/avatar.gif"
               alt="hxfeng" />
          <p class="site-author-name" itemprop="name">hxfeng</p>
           
              <p class="site-description motion-element" itemprop="description">这是我的博客，记录我的工作和成长。</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/blog/archives">
                <span class="site-state-item-count">21</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/blog/categories/index.html">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/blog/tags/index.html">
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据显示"><span class="nav-number">1.</span> <span class="nav-text">数据显示</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#将dataSet转换成dataFrame"><span class="nav-number">2.</span> <span class="nav-text">将dataSet转换成dataFrame</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据展示"><span class="nav-number">3.</span> <span class="nav-text">数据展示</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据的关联"><span class="nav-number">4.</span> <span class="nav-text">数据的关联</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#排序分组"><span class="nav-number">5.</span> <span class="nav-text">排序分组</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#别名"><span class="nav-number">6.</span> <span class="nav-text">别名</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#查询"><span class="nav-number">7.</span> <span class="nav-text">查询</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#过滤"><span class="nav-number">8.</span> <span class="nav-text">过滤</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#分组查询"><span class="nav-number">9.</span> <span class="nav-text">分组查询</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reduce操作"><span class="nav-number">10.</span> <span class="nav-text">reduce操作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据钻取与聚合操作"><span class="nav-number">11.</span> <span class="nav-text">数据钻取与聚合操作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#集合的交并补接口"><span class="nav-number">12.</span> <span class="nav-text">集合的交并补接口</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#取样与切分"><span class="nav-number">13.</span> <span class="nav-text">取样与切分</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#去重"><span class="nav-number">14.</span> <span class="nav-text">去重</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#统计指定的列"><span class="nav-number">15.</span> <span class="nav-text">统计指定的列</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据的转换"><span class="nav-number">16.</span> <span class="nav-text">数据的转换</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据的提取与聚合"><span class="nav-number">17.</span> <span class="nav-text">数据的提取与聚合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#分区"><span class="nav-number">18.</span> <span class="nav-text">分区</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据的持久化"><span class="nav-number">19.</span> <span class="nav-text">数据的持久化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#注册临时表"><span class="nav-number">20.</span> <span class="nav-text">注册临时表</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据保存"><span class="nav-number">21.</span> <span class="nav-text">数据保存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#转换成json格式"><span class="nav-number">22.</span> <span class="nav-text">转换成json格式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#获取文件列表"><span class="nav-number">23.</span> <span class="nav-text">获取文件列表</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">hxfeng</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hxfeng.github.io/blog">hxfeng</a> 强力驱动
</div>

<div class="theme-info">
 <!-- 主题 - -->
  <a class="theme-link" href="https://hxfeng.github.io">
   <!-- NexT.Pisces -->
   我的博客
  </a>
</div>


        

<div class="busuanzi-count">

  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv"><i class="fa fa-user"></i> 访问人数<span class="busuanzi-value" id="busuanzi_value_site_uv"></span></span>
  

  
    <span class="site-pv"><i class="fa fa-eye"></i> 访问总量<span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span>
  
  
</div>



        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    
    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/blog/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/blog/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/blog/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/blog/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/blog/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/blog/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/blog/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/blog/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/blog/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/blog/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/blog/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/blog/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/blog/js/src/bootstrap.js?v=5.1.0"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"hxfengds"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    
    <script src="/blog/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/blog/js/src/hook-duoshuo.js"></script>
  













  
  

  
  


  

  
<script>
var _hmt = _hmt || [];
(function() {
      var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?f461cb82049b156a480ab29cdb6d83bc";
          var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
})();
</script>


  


  

</body>
</html>
